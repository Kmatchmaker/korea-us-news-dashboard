import re
import hashlib
from datetime import datetime
from urllib.parse import quote, urljoin

import pandas as pd
import requests
import streamlit as st
import feedparser
import yaml
from bs4 import BeautifulSoup
from dateutil import parser as dateparser


# -----------------------------
# Config
# -----------------------------
CONFIG_PATH = "config.yaml"
USER_AGENT = "Mozilla/5.0 (StreamlitNewsBoard/1.0)"
HEADERS = {"User-Agent": USER_AGENT}

# ê¸°ë³¸ í‘œì‹œ ì—°ë„(ì›í•˜ë©´ 2026 -> "ì „ì²´" ë¡œ ë°”ê¿”ë„ ë¨)
DEFAULT_YEAR_FILTER = 2026


def load_config():
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


cfg = load_config()

states_cfg = cfg.get("states", {})
companies = cfg.get("companies", [])
korean_queries = cfg.get("korean_queries", [])
us_sources = cfg.get("us_sources", [])


# -----------------------------
# Utilities
# -----------------------------
_ws = re.compile(r"\s+")


def norm_text(s: str) -> str:
    return _ws.sub(" ", (s or "").strip())


def norm_query_for_url(q: str) -> str:
    # ì¤„ë°”ê¿ˆ/ì—¬ëŸ¬ ê³µë°± ì œê±° -> URL-safe encode
    q = norm_text(q)
    return quote(q)


def safe_parse_date(s: str):
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        return dt
    except Exception:
        return None


def detect_state(text: str) -> str:
    t = norm_text(text).lower()
    for code, names in (states_cfg or {}).items():
        # config.yamlì—ì„œ statesë¥¼ ["Georgia", ...] í˜•íƒœë¡œ ë’€ì„ ìˆ˜ë„ ìˆê³ 
        # {names:[...]} í˜•íƒœì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ë‘˜ ë‹¤ ì§€ì›
        if isinstance(names, dict):
            names_list = names.get("names", [])
        else:
            names_list = names

        for n in names_list:
            if norm_text(str(n)).lower() in t:
                return code
    return "Global"


def detect_company(text: str) -> str:
    t = norm_text(text)
    for c in sorted(companies, key=len, reverse=True):
        if c and c in t:
            return c
    return "Unknown"


def make_core(title: str, summary: str | None = None) -> str:
    s = norm_text(summary or "")
    if s:
        return (s[:180] + "â€¦") if len(s) > 180 else s
    t = norm_text(title)
    return (t[:180] + "â€¦") if len(t) > 180 else t


def make_id(title: str, url: str, provider: str) -> str:
    raw = f"{provider}||{norm_text(title)}||{norm_text(url)}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def to_display_date(dt) -> str:
    if not dt:
        return ""
    try:
        return dt.strftime("%Y.%m.%d")
    except Exception:
        return ""


def link_md(label: str, url: str) -> str:
    return f"[{label}]({url})"


# -----------------------------
# Fetch: Korean (Google News RSS)
# -----------------------------
@st.cache_data(ttl=60 * 30)  # 30ë¶„ ìºì‹œ(ìŠ¤íŠ¸ë¦¼ë¦¿ ì¬ì‹¤í–‰ ì‹œ ê³¼ë„í•œ í˜¸ì¶œ ë°©ì§€)
def fetch_korean_news(limit_per_query: int = 20):
    results = []

    for q in korean_queries:
        q_encoded = norm_query_for_url(q)
        url = f"https://news.google.com/rss/search?q={q_encoded}&hl=ko&gl=KR&ceid=KR:ko"

        feed = feedparser.parse(url)

        for e in feed.entries[:limit_per_query]:
            title = norm_text(getattr(e, "title", ""))
            link = norm_text(getattr(e, "link", ""))
            published_raw = getattr(e, "published", "") or getattr(e, "updated", "")
            dt = safe_parse_date(published_raw)

            summary = getattr(e, "summary", None) or getattr(e, "description", None)

            if not title or not link:
                continue

            results.append(
                {
                    "provider": "KOREAN",
                    "source": "Google News (KR)",
                    "state": detect_state(title),
                    "company": detect_company(title),
                    "published_at": dt,
                    "core": make_core(title, summary),
                    "title": title,
                    "url": link,
                }
            )

    # ì¤‘ë³µ ì œê±°
    dedup = {}
    for r in results:
        k = make_id(r["title"], r["url"], r["provider"])
        dedup[k] = r

    return list(dedup.values())


# -----------------------------
# Fetch: US sources (HTML list)
# -----------------------------
def guess_items_from_page(html: str, base_url: str, max_items: int = 40):
    soup = BeautifulSoup(html, "html.parser")
    items = []

    # 1) article ê¸°ë°˜
    for art in soup.select("article"):
        a = art.select_one("a[href]")
        if not a:
            continue
        title = norm_text(a.get_text(" "))
        href = norm_text(a.get("href", ""))
        if not title or not href:
            continue
        full_url = urljoin(base_url, href)

        # ë‚ ì§œ í›„ë³´
        date_text = None
        time_tag = art.select_one("time")
        if time_tag:
            date_text = norm_text(time_tag.get("datetime") or time_tag.get_text(" "))

        items.append((title, full_url, date_text))

    # 2) ê·¸ë˜ë„ ì—†ìœ¼ë©´ ë§í¬ ë¦¬ìŠ¤íŠ¸
    if not items:
        for a in soup.select("a[href]"):
            title = norm_text(a.get_text(" "))
            href = norm_text(a.get("href", ""))
            if not title or not href:
                continue
            if len(title) < 12:  # ë„ˆë¬´ ì§§ì€ ë©”ë‰´/ë‚´ë¹„ê²Œì´ì…˜ ì œì™¸
                continue
            full_url = urljoin(base_url, href)
            items.append((title, full_url, None))

    # ì¤‘ë³µ ì œê±° + ìƒìœ„ Nê°œ
    seen = set()
    out = []
    for t, u, d in items:
        key = (t, u)
        if key in seen:
            continue
        seen.add(key)
        out.append((t, u, d))
        if len(out) >= max_items:
            break
    return out


@st.cache_data(ttl=60 * 30)
def fetch_us_news(max_items_per_source: int = 30):
    results = []
    for src in us_sources:
        name = src.get("name", "US Source")
        url = src.get("url")
        if not url:
            continue

        try:
            r = requests.get(url, headers=HEADERS, timeout=20)
            r.raise_for_status()
            items = guess_items_from_page(r.text, url, max_items=max_items_per_source)
        except Exception:
            continue

        for title, link, date_text in items:
            dt = safe_parse_date(date_text) if date_text else None
            results.append(
                {
                    "provider": "US",
                    "source": name,
                    "state": detect_state(title),
                    "company": detect_company(title),
                    "published_at": dt,
                    "core": make_core(title, None),
                    "title": title,
                    "url": link,
                }
            )

    # ì¤‘ë³µ ì œê±°
    dedup = {}
    for r in results:
        k = make_id(r["title"], r["url"], r["provider"])
        dedup[k] = r

    return list(dedup.values())


# -----------------------------
# Render
# -----------------------------
def build_table(rows):
    if not rows:
        return pd.DataFrame(columns=["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ì›ë¬¸ í™•ì¸"])

    df = pd.DataFrame(rows)

    # ë°œí–‰ì¼ ì²˜ë¦¬: ì—†ìœ¼ë©´ ë¹ˆê°’
    df["ë‰´ìŠ¤ ë°œí–‰ì¼"] = df["published_at"].apply(to_display_date)

    # í‘œ ì»¬ëŸ¼ ë§¤í•‘
    df["ì£¼(State)"] = df["state"]
    df["ê¸°ì—…ëª…"] = df["company"]
    df["í•µì‹¬ ë‚´ìš©"] = df["core"]
    df["ì›ë¬¸ í™•ì¸"] = df.apply(lambda r: link_md(r["source"], r["url"]), axis=1)

    # ì‚¬ìš©ìê°€ í´ë¦­í•˜ê¸° ì¢‹ì€ "ê¸°ì‚¬ ì œëª©"ë„ ë³´ì¡°ë¡œ ì œê³µ
    df["ê¸°ì‚¬ ì œëª©"] = df.apply(lambda r: link_md(r["title"], r["url"]), axis=1)

    return df[["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ê¸°ì‚¬ ì œëª©", "ì›ë¬¸ í™•ì¸"]]


def apply_filters(df: pd.DataFrame, year_filter, state_filter, company_text, keyword_text):
    out = df.copy()

    # ì—°ë„ í•„í„°
    if year_filter != "ì „ì²´":
        y = int(year_filter)
        # ë‰´ìŠ¤ ë°œí–‰ì¼ì´ ë¹ˆê°’ì´ë©´ ì œì™¸
        out = out[out["ë‰´ìŠ¤ ë°œí–‰ì¼"].str.startswith(str(y))]

    # ì£¼ í•„í„°
    if state_filter != "ì „ì²´":
        out = out[out["ì£¼(State)"] == state_filter]

    # ê¸°ì—… í•„í„°
    if company_text.strip():
        c = company_text.strip().lower()
        out = out[
            out["ê¸°ì—…ëª…"].str.lower().str.contains(c, na=False)
            | out["ê¸°ì‚¬ ì œëª©"].str.lower().str.contains(c, na=False)
        ]

    # í‚¤ì›Œë“œ í•„í„°
    if keyword_text.strip():
        k = keyword_text.strip().lower()
        out = out[
            out["í•µì‹¬ ë‚´ìš©"].str.lower().str.contains(k, na=False)
            | out["ê¸°ì‚¬ ì œëª©"].str.lower().str.contains(k, na=False)
        ]

    return out


# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="ë¯¸êµ­ ë™ë‚¨ë¶€ ì§„ì¶œ í•œêµ­ê¸°ì—… ë‰´ìŠ¤ ìƒí™©íŒ", layout="wide")
st.title("ğŸ“° ë¯¸êµ­ ë™ë‚¨ë¶€(GA/TN/AL/SC/FL) ì§„ì¶œ í•œêµ­ê¸°ì—… ë‰´ìŠ¤ ìƒí™©íŒ")
st.caption("íƒ­ìœ¼ë¡œ í•œêµ­ì–´ ë‰´ìŠ¤ vs ë¯¸êµ­(ì£¼ì •ë¶€/ê¸°ê´€) ì†ŒìŠ¤ë¥¼ ë¶„ë¦¬í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤. (GitHubë§Œìœ¼ë¡œ ìš´ì˜ ê°€ëŠ¥)")

with st.sidebar:
    st.subheader("í•„í„°")
    year_filter = st.selectbox("ë°œí–‰ ì—°ë„", [DEFAULT_YEAR_FILTER, 2025, 2024, "ì „ì²´"], index=0)
    state_filter = st.selectbox("ì£¼(State)", ["ì „ì²´", "GA", "TN", "AL", "SC", "FL", "Global"], index=0)
    company_text = st.text_input("ê¸°ì—…ëª… ê²€ìƒ‰(ë¶€ë¶„)", "")
    keyword_text = st.text_input("í‚¤ì›Œë“œ(ì œëª©/ë‚´ìš©)", "")

    st.markdown("---")
    if st.button("ğŸ”„ ìºì‹œ ìƒˆë¡œê³ ì¹¨(ê°•ì œ ì¬ìˆ˜ì§‘)"):
        st.cache_data.clear()
        st.rerun()

tab1, tab2 = st.tabs(["ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë‰´ìŠ¤", "ğŸ‡ºğŸ‡¸ ë¯¸êµ­(ì£¼ì •ë¶€Â·ê¸°ê´€) ë‰´ìŠ¤"])

with tab1:
    st.subheader("í•œêµ­ì–´ ë‰´ìŠ¤ (Google News RSS)")
    rows = fetch_korean_news()
    df = build_table(rows)
    df2 = apply_filters(df, year_filter, state_filter, company_text, keyword_text)
    st.caption(f"í‘œì‹œ: {len(df2)}ê±´ / ì „ì²´ ìˆ˜ì§‘: {len(df)}ê±´")
    st.dataframe(df2, use_container_width=True, hide_index=True)

with tab2:
    st.subheader("ë¯¸êµ­ ì†ŒìŠ¤ (ì£¼ì •ë¶€/ê¸°ê´€ ì›¹í˜ì´ì§€)")
    rows = fetch_us_news()
    df = build_table(rows)
    df2 = apply_filters(df, year_filter, state_filter, company_text, keyword_text)
    st.caption(f"í‘œì‹œ: {len(df2)}ê±´ / ì „ì²´ ìˆ˜ì§‘: {len(df)}ê±´")
    st.dataframe(df2, use_container_width=True, hide_index=True)

st.markdown("---")
st.write(
    "íŒ: ë¯¸êµ­ ì†ŒìŠ¤ëŠ” ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë°”ë€Œë©´ ë§í¬ ì¶”ì¶œì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ìš”. "
    "ê·¸ëŸ´ ë• í•´ë‹¹ ì†ŒìŠ¤ë§Œ 'ì „ìš© íŒŒì„œ'ë¡œ ë§ì¶¤ ì²˜ë¦¬í•˜ë©´ ì •í™•ë„ê°€ í¬ê²Œ ì˜¬ë¼ê°‘ë‹ˆë‹¤."
)
