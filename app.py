import re
import hashlib
from datetime import datetime, timezone
from urllib.parse import quote, urljoin

import pandas as pd
import requests
import streamlit as st
import feedparser
import yaml
from bs4 import BeautifulSoup
from dateutil import parser as dateparser


# -----------------------------
# Settings
# -----------------------------
CONFIG_PATH = "config.yaml"
USER_AGENT = "Mozilla/5.0 (StreamlitNewsBoard/2.0)"
HEADERS = {"User-Agent": USER_AGENT}

DEFAULT_YEAR_FILTER = 2026          # ê¸°ë³¸ 2026ë…„ë§Œ ë³´ì—¬ì£¼ê¸° (ì›í•˜ë©´ "ì „ì²´"ë¡œ ë°”ê¾¸ë©´ ë¨)
TOP_OTHER_UPDATES = 12              # ëŒ€ê¸°ì—… ì™¸ 'ê¸°íƒ€ ì£¼ìš” ì—…ë°ì´íŠ¸' ë…¸ì¶œ ê°œìˆ˜


# -----------------------------
# Load config
# -----------------------------
def load_config():
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


cfg = load_config()
states_cfg = cfg.get("states", {})
companies = cfg.get("companies", [])
priority_companies = cfg.get("priority_companies", [])  # << ì¶”ê°€
korean_queries = cfg.get("korean_queries", [])
us_sources = cfg.get("us_sources", [])


# -----------------------------
# Utilities
# -----------------------------
_ws = re.compile(r"\s+")
_html_tag = re.compile(r"<[^>]+>")


def norm_text(s: str) -> str:
    return _ws.sub(" ", (s or "").strip())


def strip_html(s: str) -> str:
    return norm_text(_html_tag.sub(" ", s or ""))


def norm_query_for_url(q: str) -> str:
    q = norm_text(q)
    return quote(q)


def safe_parse_date(s: str):
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        if dt and not dt.tzinfo:
            # timezone ì—†ëŠ” ê²½ìš° UTCë¡œ ê°€ì •
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None


def to_display_date(dt) -> str:
    if not dt:
        return ""
    try:
        return dt.astimezone(timezone.utc).strftime("%Y.%m.%d")
    except Exception:
        try:
            return dt.strftime("%Y.%m.%d")
        except Exception:
            return ""


def link_md(label: str, url: str) -> str:
    return f"[{label}]({url})"


def make_id(provider: str, title: str, url: str) -> str:
    raw = f"{provider}||{norm_text(title)}||{norm_text(url)}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def detect_state(text: str) -> str:
    t = norm_text(text).lower()
    for code, names in (states_cfg or {}).items():
        if isinstance(names, dict):
            names_list = names.get("names", [])
        else:
            names_list = names
        for n in names_list:
            if norm_text(str(n)).lower() in t:
                return code
    return "Global"


def detect_company(text: str) -> str:
    t = norm_text(text)
    # ê¸´ ì´ë¦„ ìš°ì„ 
    for c in sorted(companies, key=len, reverse=True):
        if c and c in t:
            return c
    return "Unknown"


# -----------------------------
# Importance scoring (rules)
# -----------------------------
INVEST_TAGS = ["íˆ¬ì", "ê³µì¥", "ì¦ì„¤", "ì„¤ë¦½", "ë²•ì¸", "ì§€ì‚¬", "ì§„ì¶œ", "ì‹ ê·œ", "í™•ì¥", "ìº í¼ìŠ¤", "ë¼ì¸"]
DEAL_TAGS = ["ìˆ˜ì£¼", "ê³„ì•½", "ê³µê¸‰", "ì²´ê²°", "MOU", "í˜‘ì•½", "íŒŒíŠ¸ë„ˆì‹­"]
STATUS_TAGS = ["ì‹¤ì ", "ë§¤ì¶œ", "íŒë§¤", "ì ìœ ìœ¨", "ê¸°ë¡", "ê°€ë™", "ìƒì‚°", "ê´€ì„¸", "ì •ì±…", "ê·œì œ", "ì „ë§", "ê°€ì´ë˜ìŠ¤"]
US_TAGS = ["ë¯¸êµ­", "ë¶ë¯¸", "U.S.", "US", "America", "Georgia", "Tennessee", "Alabama", "Florida", "South Carolina", "GA", "TN", "AL", "FL", "SC"]


def importance_score(row: dict) -> int:
    title = row.get("title", "") or ""
    core = row.get("core", "") or ""
    text = f"{title} {core}"

    score = 0
    company = row.get("company", "Unknown")

    if company in priority_companies:
        score += 100
    if any(k in text for k in INVEST_TAGS):
        score += 35
    if any(k in text for k in DEAL_TAGS):
        score += 25
    if any(k in text for k in STATUS_TAGS):
        score += 15
    if any(k in text for k in US_TAGS):
        score += 10

    # í•œêµ­ì–´ ê¸°ì‚¬(ë³´í†µ ìš”ì•½ í’ˆì§ˆì´ ì¢‹ìŒ) ì•½ê°„ ê°€ì 
    if row.get("provider") == "KOREAN":
        score += 5

    return score


# -----------------------------
# Korean summary extraction (no LLM)
# - í•œêµ­ì–´ ê¸°ì‚¬: RSS summary/description + ì œëª© ê¸°ë°˜ìœ¼ë¡œ í•œ ì¤„ ìš”ì•½
# - ì˜ì–´ ê¸°ì‚¬: title/ë©”íƒ€ì„¤ëª… ê·¸ëŒ€ë¡œ (ë²ˆì—­ API ì—†ìœ¼ë©´ ì™„ë²½í•œ í•œê¸€í™” ë¶ˆê°€)
# -----------------------------
def make_korean_core(title: str, summary_html: str | None) -> str:
    summary = strip_html(summary_html or "")
    if summary:
        return (summary[:180] + "â€¦") if len(summary) > 180 else summary
    t = norm_text(title)
    return (t[:180] + "â€¦") if len(t) > 180 else t


def fetch_meta_description(url: str) -> str | None:
    # US ì†ŒìŠ¤ ë“±ì—ì„œ "í•µì‹¬ë‚´ìš©"ì„ ì¡°ê¸ˆì´ë¼ë„ í™•ë³´í•˜ê¸° ìœ„í•´ og:description/meta description ì‚¬ìš©
    try:
        r = requests.get(url, headers=HEADERS, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        og = soup.select_one('meta[property="og:description"]')
        if og and og.get("content"):
            return norm_text(og.get("content"))
        md = soup.select_one('meta[name="description"]')
        if md and md.get("content"):
            return norm_text(md.get("content"))
        return None
    except Exception:
        return None


# -----------------------------
# Fetch: Korean (Google News RSS)
# -----------------------------
@st.cache_data(ttl=60 * 20)
def fetch_korean_news():
    rows = []

    for q in korean_queries:
        q_encoded = norm_query_for_url(q)
        url = f"https://news.google.com/rss/search?q={q_encoded}&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(url)

        for e in feed.entries[:40]:
            title = norm_text(getattr(e, "title", ""))
            link = norm_text(getattr(e, "link", ""))
            published_raw = getattr(e, "published", "") or getattr(e, "updated", "")
            dt = safe_parse_date(published_raw)
            summary = getattr(e, "summary", None) or getattr(e, "description", None)

            if not title or not link:
                continue

            row = {
                "provider": "KOREAN",
                "source": "Google News (KR)",
                "title": title,
                "url": link,
                "published_at": dt,
                "state": detect_state(title),
                "company": detect_company(title),
                "core": make_korean_core(title, summary),
            }
            rows.append(row)

    # dedup
    dedup = {}
    for r in rows:
        dedup[make_id(r["provider"], r["title"], r["url"])] = r
    return list(dedup.values())


# -----------------------------
# Fetch: US sources (HTML list + meta description)
# -----------------------------
def guess_items_from_page(html: str, base_url: str, max_items: int = 60):
    soup = BeautifulSoup(html, "html.parser")
    items = []

    for art in soup.select("article"):
        a = art.select_one("a[href]")
        if not a:
            continue
        title = norm_text(a.get_text(" "))
        href = norm_text(a.get("href", ""))
        if not title or not href:
            continue
        full_url = urljoin(base_url, href)

        date_text = None
        time_tag = art.select_one("time")
        if time_tag:
            date_text = norm_text(time_tag.get("datetime") or time_tag.get_text(" "))

        items.append((title, full_url, date_text))

    if not items:
        for a in soup.select("a[href]"):
            title = norm_text(a.get_text(" "))
            href = norm_text(a.get("href", ""))
            if not title or not href:
                continue
            if len(title) < 12:
                continue
            full_url = urljoin(base_url, href)
            items.append((title, full_url, None))

    seen = set()
    out = []
    for t, u, d in items:
        key = (t, u)
        if key in seen:
            continue
        seen.add(key)
        out.append((t, u, d))
        if len(out) >= max_items:
            break
    return out


@st.cache_data(ttl=60 * 30)
def fetch_us_news():
    rows = []
    for src in us_sources:
        name = src.get("name", "US Source")
        url = src.get("url")
        if not url:
            continue

        try:
            r = requests.get(url, headers=HEADERS, timeout=20)
            r.raise_for_status()
            items = guess_items_from_page(r.text, url, max_items=50)
        except Exception:
            continue

        for title, link, date_text in items:
            dt = safe_parse_date(date_text) if date_text else None

            # í•µì‹¬ë‚´ìš©ì„ ì¡°ê¸ˆì´ë¼ë„ í™•ë³´ (ë©”íƒ€ ì„¤ëª…)
            meta_desc = fetch_meta_description(link)
            core = meta_desc if meta_desc else title

            row = {
                "provider": "US",
                "source": name,
                "title": title,
                "url": link,
                "published_at": dt,
                "state": detect_state(title),
                "company": detect_company(title),
                "core": core,
            }
            rows.append(row)

    dedup = {}
    for r in rows:
        dedup[make_id(r["provider"], r["title"], r["url"])] = r
    return list(dedup.values())


# -----------------------------
# Build & filter
# -----------------------------
def build_df(rows: list[dict]) -> pd.DataFrame:
    if not rows:
        return pd.DataFrame(columns=["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ì›ë¬¸ í™•ì¸", "_when", "_score", "_provider"])

    df = pd.DataFrame(rows)
    df["_when"] = df["published_at"].apply(lambda x: x if x else None)
    df["_when"] = pd.to_datetime(df["_when"], errors="coerce", utc=True)

    # ë‚ ì§œ ì—†ëŠ” ê±´ ìµœê·¼ì„± ì •ë ¬ì—ì„œ ë°€ë¦¬ê²Œ ì²˜ë¦¬
    now = pd.Timestamp.now(tz="UTC")
    df["_when"] = df["_when"].fillna(now - pd.Timedelta(days=3650))

    df["_score"] = df.apply(lambda r: importance_score(r.to_dict()), axis=1)

    df["ë‰´ìŠ¤ ë°œí–‰ì¼"] = df["_when"].dt.strftime("%Y.%m.%d")
    df["ì£¼(State)"] = df["state"]
    df["ê¸°ì—…ëª…"] = df["company"]
    df["í•µì‹¬ ë‚´ìš©"] = df["core"]
    df["ì›ë¬¸ í™•ì¸"] = df.apply(lambda r: link_md(r["source"], r["url"]), axis=1)
    df["_provider"] = df["provider"]

    # ë³´ê¸° ì¢‹ì€ ë§í¬(ì œëª© í´ë¦­)
    df["ê¸°ì‚¬ ì œëª©"] = df.apply(lambda r: link_md(r["title"], r["url"]), axis=1)

    # ì •ë ¬: ìµœê·¼ ìš°ì„ , ì¤‘ìš”ë„ ìš°ì„ 
    df = df.sort_values(by=["_when", "_score"], ascending=[False, False])
    return df


def apply_year_state_filters(df: pd.DataFrame, year_filter, state_filter):
    out = df.copy()
    if year_filter != "ì „ì²´":
        y = int(year_filter)
        out = out[out["ë‰´ìŠ¤ ë°œí–‰ì¼"].str.startswith(str(y))]
    if state_filter != "ì „ì²´":
        out = out[out["ì£¼(State)"] == state_filter]
    return out


def pick_top_one_per_company(df: pd.DataFrame, company_list: list[str]) -> pd.DataFrame:
    # ìš°ì„  ê¸°ì—… ë¦¬ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” ê²ƒë§Œ
    subset = df[df["ê¸°ì—…ëª…"].isin(company_list)].copy()
    if subset.empty:
        return subset
    # ê¸°ì—…ë‹¹ 1ê°œ(ê°€ì¥ ìµœê·¼/ì¤‘ìš”)
    top = subset.sort_values(by=["_when", "_score"], ascending=[False, False]).groupby("ê¸°ì—…ëª…", as_index=False).head(1)
    # ê¸°ì—…ë¦¬ìŠ¤íŠ¸ ìˆœì„œëŒ€ë¡œ ë³´ì´ê²Œ
    order_map = {c: i for i, c in enumerate(company_list)}
    top["_order"] = top["ê¸°ì—…ëª…"].map(lambda x: order_map.get(x, 9999))
    return top.sort_values(by=["_order"], ascending=True).drop(columns=["_order"])


def pick_other_updates(df: pd.DataFrame, exclude_companies: list[str], n: int) -> pd.DataFrame:
    other = df[~df["ê¸°ì—…ëª…"].isin(exclude_companies)].copy()
    if other.empty:
        return other
    # Unknown íšŒì‚¬ë¼ë„ íˆ¬ì/ì§„ì¶œ ê¸°ì‚¬ë©´ ë‚¨ê¸°ê³  ì‹¶ì–´ì„œ score ê¸°ë°˜ ìœ ì§€
    other = other.sort_values(by=["_when", "_score"], ascending=[False, False]).head(n)
    return other


# -----------------------------
# UI
# -----------------------------
st.set_page_config(page_title="ë¯¸êµ­ ë™ë‚¨ë¶€ ì§„ì¶œ í•œêµ­ê¸°ì—… ë‰´ìŠ¤ ìƒí™©íŒ", layout="wide")
st.title("ğŸ“° ë¯¸êµ­ ë™ë‚¨ë¶€ ì§„ì¶œ í•œêµ­ê¸°ì—… ë‰´ìŠ¤ ìƒí™©íŒ")
st.caption("ëŒ€ê¸°ì—…ì€ ê¸°ì—…ë‹¹ 1ê°œ(ìµœì‹ /ì¤‘ìš”)ë§Œ ë³´ì—¬ì£¼ê³ , ê·¸ ì™¸ ì‹ ê·œ íˆ¬ì/ì§„ì¶œ/ë¯¸êµ­ì‚¬ì—… í˜„í™©ë„ ìµœì‹ ìˆœìœ¼ë¡œ ë³„ë„ í‘œì‹œí•©ë‹ˆë‹¤.")

with st.sidebar:
    st.subheader("í•„í„°")
    year_filter = st.selectbox("ë°œí–‰ ì—°ë„", [DEFAULT_YEAR_FILTER, 2025, 2024, "ì „ì²´"], index=0)
    state_filter = st.selectbox("ì£¼(State)", ["ì „ì²´", "GA", "TN", "AL", "SC", "FL", "Global"], index=0)

    st.markdown("---")
    st.write("ëŒ€ê¸°ì—…(ìš°ì„  í‘œì‹œ):")
    st.code(", ".join(priority_companies) if priority_companies else "(config.yamlì— priority_companies ì¶”ê°€ í•„ìš”)")

    if st.button("ğŸ”„ ìºì‹œ ìƒˆë¡œê³ ì¹¨(ê°•ì œ ì¬ìˆ˜ì§‘)"):
        st.cache_data.clear()
        st.rerun()

tab1, tab2 = st.tabs(["ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë‰´ìŠ¤", "ğŸ‡ºğŸ‡¸ ë¯¸êµ­(ì£¼ì •ë¶€Â·ê¸°ê´€/ì–¸ë¡ )"])

def render(provider: str):
    if provider == "KOREAN":
        rows = fetch_korean_news()
    else:
        rows = fetch_us_news()

    df = build_df(rows)
    df = apply_year_state_filters(df, year_filter, state_filter)

    # 1) ëŒ€ê¸°ì—…: ê¸°ì—…ë‹¹ 1ê°œ
    st.subheader("â­ ëŒ€ê¸°ì—… ìµœì‹  í•µì‹¬ ë‰´ìŠ¤ (ê¸°ì—…ë‹¹ 1ê°œ)")
    top_big = pick_top_one_per_company(df, priority_companies)
    if top_big.empty:
        st.info("í•´ë‹¹ ì¡°ê±´ì—ì„œ ëŒ€ê¸°ì—… ë‰´ìŠ¤ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. (ì¿¼ë¦¬/ê¸°ì—…ëª… ë§¤ì¹­ì„ í™•ì¥í•´ë³´ì„¸ìš”)")
    else:
        st.dataframe(
            top_big[["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ê¸°ì‚¬ ì œëª©", "ì›ë¬¸ í™•ì¸"]],
            use_container_width=True,
            hide_index=True,
        )

    # 2) ê¸°íƒ€ ì—…ë°ì´íŠ¸: íˆ¬ì/ì§„ì¶œ/ë¯¸êµ­ ì‚¬ì—…í˜„í™© ë“±
    st.subheader("ğŸ†• ì‹ ê·œ íˆ¬ìÂ·ì§„ì¶œ ë° ë¯¸êµ­ ì‚¬ì—… í˜„í™© (ìµœì‹ )")
    other = pick_other_updates(df, exclude_companies=priority_companies, n=TOP_OTHER_UPDATES)
    if other.empty:
        st.info("í•´ë‹¹ ì¡°ê±´ì—ì„œ ì¶”ê°€ ì£¼ìš” ì—…ë°ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.dataframe(
            other[["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ê¸°ì‚¬ ì œëª©", "ì›ë¬¸ í™•ì¸"]],
            use_container_width=True,
            hide_index=True,
        )

    st.caption(f"ìˆ˜ì§‘ ê±´ìˆ˜: {len(df)} (í•„í„° ì ìš© í›„)")

with tab1:
    st.write("í•œêµ­ì–´ ë‰´ìŠ¤ëŠ” RSS ìš”ì•½/ë©”íƒ€ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ â€˜í•µì‹¬ ë‚´ìš©â€™ì´ ëŒ€ì²´ë¡œ í•œêµ­ì–´ë¡œ ì˜ ë‚˜ì˜µë‹ˆë‹¤.")
    render("KOREAN")

with tab2:
    st.write("ë¯¸êµ­ ì†ŒìŠ¤ëŠ” ì‚¬ì´íŠ¸ë§ˆë‹¤ ìš”ì•½ì´ ì˜ì–´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤(og:description ê¸°ë°˜). í•„ìš”í•˜ë©´ ë²ˆì—­ ì˜µì…˜ì„ ì¶”ê°€í•  ìˆ˜ ìˆì–´ìš”.")
    render("US")
