import re
import hashlib
from urllib.parse import urljoin, urlparse
from datetime import timezone

import pandas as pd
import requests
import streamlit as st
import yaml
from bs4 import BeautifulSoup
from dateutil import parser as dateparser


# ============================
# SETTINGS
# ============================
CONFIG_PATH = "config.yaml"
USER_AGENT = "Mozilla/5.0 (StreamlitNewsBoard/USGovOnly/1.0)"
HEADERS = {"User-Agent": USER_AGENT}

CACHE_TTL_SEC = 60 * 20
DEFAULT_YEAR_FILTER = 2026

TOP_COMPANY_MAX = 10        # ê¸°ì—…ë‹¹ 1ê°œ ìµœì‹ /ì¤‘ìš”, ìµœëŒ€ 10ê°œ ê¸°ì—…ë§Œ
OTHER_MAX = 20              # ë‚˜ë¨¸ì§€ ì—…ë°ì´íŠ¸ ëª©ë¡


# ============================
# LOAD CONFIG
# ============================
def load_config():
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


cfg = load_config()
states_cfg = cfg.get("states", {})  # GA/TN/AL/SC/FL ë“±
priority_companies = cfg.get("priority_companies", ["í˜„ëŒ€", "SK", "LG", "í•œí™”", "ê³ ë ¤ì•„ì—°"])
korean_queries = cfg.get("korean_queries", [])
us_sources = cfg.get("us_sources", [])


# ============================
# TEXT UTILS
# ============================
_ws = re.compile(r"\s+")
_html_tag = re.compile(r"<[^>]+>")


def norm_text(s: str) -> str:
    return _ws.sub(" ", (s or "").strip())


def strip_html(s: str) -> str:
    return norm_text(_html_tag.sub(" ", s or ""))


def safe_parse_date(s: str):
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        if dt and not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None


def make_id(provider: str, title: str, url: str) -> str:
    raw = f"{provider}||{norm_text(title)}||{norm_text(url)}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


# ============================
# STATE DETECTION (ì˜¤íƒ ì¤„ì´ê¸°)
# - "SC" ê°™ì€ ì•½ì–´ëŠ” ë‹¨ì–´ ê²½ê³„ë¡œë§Œ ì¸ì‹
# - "Georgia"ëŠ” ì£¼ì •ë¶€ ì‚¬ì´íŠ¸ì—ì„œë§Œ ë‚˜ì˜¤ê²Œ í•  ê±°ë¼ í¬ê²Œ ë¬¸ì œ ê°ì†Œ
# ============================
STATE_ABBR = ["GA", "TN", "AL", "SC", "FL"]
STATE_ABBR_RE = {abbr: re.compile(rf"(?<![A-Z0-9]){abbr}(?![A-Z0-9])") for abbr in STATE_ABBR}


def detect_state_strict(text: str, source_url: str) -> str:
    t = norm_text(text)

    # 1) ê¸´ ì´ë¦„ ë¨¼ì € (South Carolina / Tennessee ë“±)
    tl = t.lower()
    for code, names in (states_cfg or {}).items():
        # config.yamlì—ì„œ namesê°€ listë¼ê³  ê°€ì •(ì´ì „ ëŒ€í™” ê¸°ì¤€)
        for n in names:
            n_norm = norm_text(str(n))
            # ì•½ì–´ëŠ” ë³„ë„ ì²˜ë¦¬
            if n_norm.upper() in STATE_ABBR:
                continue
            if n_norm and n_norm.lower() in tl:
                return code

    # 2) ì•½ì–´ëŠ” "ë‹¨ë… í† í°"ë§Œ
    for abbr, rx in STATE_ABBR_RE.items():
        if rx.search(t):
            return abbr

    # 3) ë„ë©”ì¸ íŒíŠ¸(ê°€ëŠ¥í•˜ë©´)
    host = (urlparse(source_url).netloc or "").lower()
    if "georgia" in host:
        return "GA"
    if "tnecd" in host or "tennessee" in host:
        return "TN"
    if "alabama" in host:
        return "AL"
    if "sccommerce" in host or "southcarolina" in host:
        return "SC"
    if "florida" in host:
        return "FL"

    return "Global"


# ============================
# COMPANY DETECTION
# - TOP5ëŠ” aliasë¡œ ë¬¶ê³ 
# - ê·¸ ì™¸ëŠ” "ê¸°ì‚¬ì— ë‚˜ì˜¨ íšŒì‚¬ëª…"ì„ ì œëª©ì—ì„œ ë½‘ì•„ í‘œì‹œ
# ============================
TOP5_ALIASES = {
    "í˜„ëŒ€": ["í˜„ëŒ€", "í˜„ëŒ€ì°¨", "Hyundai", "ê¸°ì•„", "Kia"],
    "SK": ["SK", "SKì˜¨", "SK hynix", "SKí•˜ì´ë‹‰ìŠ¤", "í•˜ì´ë‹‰ìŠ¤", "SK Innovation", "SKì´ë…¸ë² ì´ì…˜"],
    "LG": ["LG", "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "LG Energy Solution", "LGí™”í•™", "LG Chem"],
    "í•œí™”": ["í•œí™”", "Hanwha", "í•œí™”íì…€", "Qcells", "Q CELLS"],
    "ê³ ë ¤ì•„ì—°": ["ê³ ë ¤ì•„ì—°", "Korea Zinc", "KoreaZinc"],
}

STOPWORDS = {
    "ë¯¸êµ­", "í•œêµ­", "ì¡°ì§€ì•„", "í…Œë„¤ì‹œ", "ì•¨ë¼ë°°ë§ˆ", "ì•Œë¼ë°°ë§ˆ", "í”Œë¡œë¦¬ë‹¤", "ì‚¬ìš°ìŠ¤ìºë¡¤ë¼ì´ë‚˜", "ìºë¡¤ë¼ì´ë‚˜",
    "íˆ¬ì", "ê³µì¥", "ì„¤ë¦½", "ì¦ì„¤", "í™•ì¥", "ì§„ì¶œ", "ê³„ì•½", "ìˆ˜ì£¼", "ê³µê¸‰", "ì²´ê²°", "ë°œí‘œ", "í™•ì •", "ìµœëŒ€",
    "ì£¼ì •ë¶€", "ì •ë¶€", "ìœ„ì›íšŒ", "ë‰´ìŠ¤", "ë³´ë„ìë£Œ", "ê²½ì œê°œë°œ", "ì¹´ìš´í‹°", "ì‹œ", "ì£¼", "ì‹œì¥", "í”„ë¡œì íŠ¸",
    "press", "release", "news", "governor", "department", "commerce", "economic", "development",
    "georgia", "tennessee", "alabama", "florida", "carolina",
}


def detect_company_from_title(title: str) -> str:
    t = norm_text(title)

    # 1) TOP5 alias ìš°ì„ 
    for canon, aliases in TOP5_ALIASES.items():
        for a in aliases:
            if a and a in t:
                return canon

    # 2) ì œëª© ë§¨ ì• í† í°(â€œOOO, â€¦â€ / â€œOOO - â€¦â€ / â€œOOO: â€¦â€)
    m = re.match(r"^([ê°€-í£A-Za-z0-9&/]+)", t)
    if m:
        cand = m.group(1)
        if len(cand) >= 2 and cand.lower() not in STOPWORDS:
            return cand

    # 3) ì œëª©ì—ì„œ íšŒì‚¬ëª… í›„ë³´ í† í° ì°¾ê¸°(í•œê¸€/ì˜ë¬¸/ìˆ«ì í˜¼í•© 2~20ì)
    # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” STOPWORDSë¡œ ê±¸ëŸ¬ëƒ„
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9&/]{2,20}", t)
    for tok in tokens:
        if tok.lower() in STOPWORDS:
            continue
        # íšŒì‚¬ëª…ì²˜ëŸ¼ ë³´ì´ë„ë¡ â€œí˜•íƒœâ€ íŒíŠ¸(ì¤‘ê³µì—…/ê¸ˆì†/ì˜¤í† /EPC ë“±) ìˆìœ¼ë©´ ìš°ì„ 
        if re.search(r"(ì¤‘ê³µì—…|ê¸ˆì†|ì˜¤í† |EPC|ì „ì|ì—ë„ˆì§€|í™”í•™|ê±´ì„¤|ëª¨ë¹Œë¦¬í‹°|í…Œí¬|ì‚°ì—…|ì†Œì¬)", tok):
            return tok
    # í˜•íƒœ íŒíŠ¸ê°€ ì—†ì–´ë„ ì²« í›„ë³´ë¥¼ ë°˜í™˜(ë„ˆê°€ ì›í•œ â€œê¸°ì‚¬ì— ë‚˜ì˜¨ ê¸°ì—…ëª…â€ ìµœëŒ€ ë°˜ì˜)
    for tok in tokens:
        if tok.lower() in STOPWORDS:
            continue
        return tok

    return "ë¯¸í™•ì¸ê¸°ì—…"


def icon_for_company(company: str) -> str:
    return "ğŸ‘‘" if company in priority_companies else "ğŸ’"


# ============================
# TAG / IMPORTANCE
# ============================
INVEST = ["invest", "investment", "plant", "facility", "expansion", "factory", "site", "build", "built", "construct"]
INVEST_KO = ["íˆ¬ì", "ê³µì¥", "ì„¤ë¦½", "ì¦ì„¤", "í™•ì¥", "ì§„ì¶œ", "ì‹ ê·œ"]
DEAL = ["contract", "deal", "supply", "agreement", "award", "wins", "signed", "signs"]
DEAL_KO = ["ìˆ˜ì£¼", "ê³„ì•½", "ê³µê¸‰", "ì²´ê²°", "í˜‘ì•½", "íŒŒíŠ¸ë„ˆì‹­"]
CAPITAL_KO = ["ì¦ì", "ì¶œì", "ê³µì‹œ"]
SALES_KO = ["íŒë§¤", "ê¸°ë¡", "ëŒíŒŒ", "ë§¤ì¶œ", "ì‹¤ì "]


def classify_tag(text: str) -> str:
    tl = text.lower()
    if any(k in text for k in INVEST_KO) or any(k in tl for k in INVEST):
        return "[ì‹ ê·œ íˆ¬ì]"
    if any(k in text for k in DEAL_KO) or any(k in tl for k in DEAL):
        return "[ìˆ˜ì£¼/ê³„ì•½]"
    if any(k in text for k in CAPITAL_KO):
        return "[ìë³¸/ê³µì‹œ]"
    if any(k in text for k in SALES_KO):
        return "[ì‹¤ì /íŒë§¤]"
    return "[ì£¼ìš”]"


def importance_score(title: str, company: str) -> int:
    score = 0
    if company in priority_companies:
        score += 100
    tag = classify_tag(title)
    if tag == "[ì‹ ê·œ íˆ¬ì]":
        score += 35
    elif tag == "[ìˆ˜ì£¼/ê³„ì•½]":
        score += 25
    elif tag == "[ìë³¸/ê³µì‹œ]":
        score += 20
    elif tag == "[ì‹¤ì /íŒë§¤]":
        score += 15
    else:
        score += 5
    return score


# ============================
# FETCH: US GOV SOURCES ONLY (HTML list)
# ============================
def guess_items_from_page(html: str, base_url: str, max_items: int = 80):
    soup = BeautifulSoup(html, "html.parser")
    host = (urlparse(base_url).netloc or "").lower()

    items = []

    # -----------------------------
    # (A) GA Governor Press: /press-releases/YYYY-MM-DD/slug íŒ¨í„´ë§Œ ì •í™•íˆ ì¶”ì¶œ
    # -----------------------------
    if "gov.georgia.gov" in host:
        for a in soup.select('a[href*="/press-releases/"]'):
            href = a.get("href", "")
            full_url = urljoin(base_url, href)
            if not re.search(r"/press-releases/\d{4}-\d{2}-\d{2}/", full_url):
                continue

            text = norm_text(a.get_text(" "))
            if not text:
                continue

            # ì˜ˆ: "Gov. Kemp: ... February 04, 2026" ê°™ì€ í•œ ì¤„ì—ì„œ ì œëª©/ë‚ ì§œ ë¶„ë¦¬
            m = re.search(r"(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}", text)
            date_text = m.group(0) if m else None
            title = text.replace(date_text, "").strip() if date_text else text

            items.append((title, full_url, date_text))

    # -----------------------------
    # (B) Georgia.org Press Releases: "Read More" ë§í¬ or ì œëª© ë¸”ë¡ì—ì„œ press-releases ë§í¬ ì¶”ì¶œ
    # -----------------------------
    elif "georgia.org" in host:
        # /press-releases ë‚´ë¶€ ë§í¬ë§Œ
        for a in soup.select('a[href*="/press-releases"]'):
            href = a.get("href", "")
            full_url = urljoin(base_url, href)

            # georgia.orgëŠ” ëª©ë¡ í•­ëª©ì´ "### Feb 4, 2026 ... Read More" í˜•íƒœë¼
            # ë‚ ì§œê°€ í…ìŠ¤íŠ¸ì— ë“¤ì–´ê°€ ìˆê±°ë‚˜, ê°™ì€ ì¹´ë“œ ì•ˆì— ìˆìŒ
            text = norm_text(a.get_text(" "))
            if not text:
                continue

            # "Read More"ë§Œ ì¡íˆëŠ” ê²½ìš°ê°€ ìˆìœ¼ë‹ˆ, ì£¼ë³€(ë¶€ëª¨)ì—ì„œ ì œëª©ì„ ëŒì–´ì˜¬ë¦¼
            if text.lower() in {"read more", "read more\u00a0"}:
                parent_text = norm_text(a.find_parent().get_text(" ")) if a.find_parent() else ""
                text = parent_text if parent_text else text

            m = re.search(r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}", text)
            date_text = m.group(0) if m else None
            title = text.replace(date_text, "").replace("Read More", "").strip()

            # ë„ˆë¬´ ì§§ì€ ì¡ìŒ ì œì™¸
            if len(title) < 12:
                continue

            items.append((title, full_url, date_text))

    # -----------------------------
    # (C) Generic fallback: article->a ìš°ì„ , ì—†ìœ¼ë©´ ì˜ë¯¸ìˆëŠ” aë§Œ
    # -----------------------------
    else:
        for art in soup.select("article"):
            a = art.select_one("a[href]")
            if not a:
                continue
            title = norm_text(a.get_text(" "))
            href = norm_text(a.get("href", ""))
            if not title or not href:
                continue
            full_url = urljoin(base_url, href)

            date_text = None
            time_tag = art.select_one("time")
            if time_tag:
                date_text = norm_text(time_tag.get("datetime") or time_tag.get_text(" "))

            items.append((title, full_url, date_text))

        if not items:
            # "ë©”ì¸ ì½˜í…ì¸ " ìœ„ì£¼ë¡œë§Œ í›‘ê¸° (ë©”ë‰´/í‘¸í„° ì¡ìŒ ì¤„ì„)
            main = soup.select_one("main") or soup
            for a in main.select("a[href]"):
                title = norm_text(a.get_text(" "))
                href = norm_text(a.get("href", ""))
                if not title or not href:
                    continue
                if len(title) < 12:
                    continue
                full_url = urljoin(base_url, href)
                items.append((title, full_url, None))

    # -----------------------------
    # Dedup + cap
    # -----------------------------
    seen = set()
    out = []
    for t, u, d in items:
        key = (t, u)
        if key in seen:
            continue
        seen.add(key)
        out.append((t, u, d))
        if len(out) >= max_items:
            break

    return out

@st.cache_data(ttl=CACHE_TTL_SEC)
def fetch_us_gov_only(sources: list[dict]):
    rows = []

    for src in sources:
        name = src.get("name", "US Government Source")
        url = src.get("url")
        if not url:
            continue

        # (ì„ íƒ) ë„ë©”ì¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸: source_urlê³¼ ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ íŠ€ëŠ” ë§í¬ëŠ” ì œì™¸
        src_host = (urlparse(url).netloc or "").lower()

        try:
            r = requests.get(url, headers=HEADERS, timeout=25)
            r.raise_for_status()
            items = guess_items_from_page(r.text, url, max_items=60)
        except Exception:
            continue

        for title, link, date_text in items:
            link_host = (urlparse(link).netloc or "").lower()

            # ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ íŠ€ëŠ” ë§í¬(ê´‘ê³ /ì™¸ë¶€ë‰´ìŠ¤) ì œê±° (ì˜¤íƒ ì¤„ì´ê¸°)
            if src_host and link_host and (src_host not in link_host):
                # ë‹¨, georgia.org ê°™ì€ ê²½ìš° press
