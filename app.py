import streamlit as st
import feedparser
import requests
import yaml
from bs4 import BeautifulSoup
from datetime import datetime

# ------------------------
# ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
# ------------------------
cfg = yaml.safe_load(open("config.yaml", encoding="utf-8"))

states = cfg["states"]
companies = cfg["companies"]

# ------------------------
# State & Company ì¶”ì¶œ í•¨ìˆ˜
# ------------------------
def detect_state(text):
    for code, names in states.items():
        for n in names:
            if n.lower() in text.lower():
                return code
    return "Global"

def detect_company(text):
    for c in companies:
        if c in text:
            return c
    return "Unknown"

# ------------------------
# í•œêµ­ ë‰´ìŠ¤ RSS ìˆ˜ì§‘
# ------------------------
def fetch_korean_news():
    results = []
    for q in cfg["korean_queries"]:
        url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(url)

        for e in feed.entries[:10]:
            title = e.title
            link = e.link
            published = getattr(e, "published", "")

            results.append({
                "State": detect_state(title),
                "Company": detect_company(title),
                "Date": published[:16],
                "Core": title,
                "Source": "í•œêµ­ë‰´ìŠ¤",
                "URL": link
            })

    return results

# ------------------------
# ë¯¸êµ­ ì£¼ì •ë¶€ ë‰´ìŠ¤ ìˆ˜ì§‘
# ------------------------
def fetch_us_news():
    results = []
    for src in cfg["us_sources"]:
        name = src["name"]
        url = src["url"]

        try:
            html = requests.get(url, timeout=10).text
            soup = BeautifulSoup(html, "html.parser")

            links = soup.select("a")[:15]

            for a in links:
                title = a.get_text().strip()
                href = a.get("href")

                if not title or not href:
                    continue

                if href.startswith("/"):
                    href = url + href

                results.append({
                    "State": detect_state(title),
                    "Company": detect_company(title),
                    "Date": datetime.today().strftime("%Y-%m-%d"),
                    "Core": title,
                    "Source": name,
                    "URL": href
                })

        except:
            continue

    return results

# ------------------------
# Streamlit UI
# ------------------------
st.set_page_config(page_title="í•œêµ­ê¸°ì—… ë¯¸êµ­ ë™ë‚¨ë¶€ ë‰´ìŠ¤ ìƒí™©íŒ", layout="wide")

st.title("ğŸ“° ë¯¸êµ­ ë™ë‚¨ë¶€ ì§„ì¶œ í•œêµ­ê¸°ì—… ë‰´ìŠ¤ ìƒí™©íŒ")
st.caption("GitHub ì›¹ì—ì„œë§Œ ê´€ë¦¬ ê°€ëŠ¥ / Streamlit ìë™ ë°°í¬")

tab1, tab2 = st.tabs(["ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë‰´ìŠ¤", "ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì£¼ì •ë¶€/ê¸°ê´€ ë‰´ìŠ¤"])

with tab1:
    st.subheader("í•œêµ­ì–´ ë‰´ìŠ¤")
    data = fetch_korean_news()
    st.dataframe(data)

with tab2:
    st.subheader("ë¯¸êµ­ ì£¼ì •ë¶€/ê¸°ê´€ ë‰´ìŠ¤")
    data = fetch_us_news()
    st.dataframe(data)
