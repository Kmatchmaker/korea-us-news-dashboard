import re
import hashlib
from datetime import datetime, timezone
from urllib.parse import quote

import pandas as pd
import streamlit as st
import feedparser
import yaml
from dateutil import parser as dateparser


# ============================
# CONFIG
# ============================
CONFIG_PATH = "config.yaml"

MAX_COMPANY_SHOW = 10   # ìµœëŒ€ 10ê°œ ê¸°ì—…ë§Œ í‘œì‹œ


# ============================
# Load config
# ============================
def load_config():
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


cfg = load_config()

companies = cfg.get("companies", [])
priority_companies = cfg.get("priority_companies", companies[:10])

korean_queries = cfg.get("korean_queries", [])

# ë¯¸êµ­ìš© ì¶”ê°€ ê²€ìƒ‰ì‹ (ê¸°ì—… ì§„ì¶œ ë‰´ìŠ¤ ë†“ì¹˜ì§€ ì•Šê¸°)
us_queries = cfg.get(
    "us_queries",
    [
        '(Georgia OR Tennessee OR Alabama OR Florida OR "South Carolina") '
        '(Korean company OR Hyundai OR Samsung OR SK OR LG OR Hanwha OR Dongwon) '
        '(investment OR plant OR expansion OR contract OR subsidiary)'
    ],
)

states_cfg = cfg.get("states", {})


# ============================
# Utils
# ============================
_ws = re.compile(r"\s+")


def norm_text(s: str) -> str:
    return _ws.sub(" ", (s or "").strip())


def safe_parse_date(s: str):
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        if dt and not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None


def detect_state(text: str) -> str:
    t = text.lower()
    for code, names in states_cfg.items():
        for n in names:
            if n.lower() in t:
                return code
    return "Global"


def detect_company(text: str) -> str:
    for c in sorted(priority_companies, key=len, reverse=True):
        if c in text:
            return c
    return "Unknown"


# ============================
# TAG ìë™ ë¶„ë¥˜
# ============================
def classify_tag(text: str) -> str:
    if any(k in text for k in ["íˆ¬ì", "ê³µì¥", "ì„¤ë¦½", "ì¦ì„¤", "ì§„ì¶œ"]):
        return "[ì‹ ê·œ íˆ¬ì]"
    if any(k in text for k in ["ìˆ˜ì£¼", "ê³„ì•½", "ê³µê¸‰", "ì²´ê²°"]):
        return "[ìˆ˜ì£¼ ëŒ€ë°•]"
    if any(k in text for k in ["ì¦ì", "ì¶œì", "ê³µì‹œ"]):
        return "[ìë³¸ ì¦ì]"
    if any(k in text for k in ["íŒë§¤", "ê¸°ë¡", "ëŒíŒŒ", "ë§¤ì¶œ"]):
        return "[íŒë§¤ ê¸°ë¡]"
    return "[ì£¼ìš” ë‰´ìŠ¤]"


# ============================
# Fetch Google News RSS
# ============================
def fetch_google_news(queries, provider="KOREAN"):
    rows = []
    for q in queries:
        q_encoded = quote(norm_text(q))
        url = f"https://news.google.com/rss/search?q={q_encoded}&hl=ko&gl=KR&ceid=KR:ko"

        feed = feedparser.parse(url)

        for e in feed.entries[:40]:
            title = norm_text(e.title)
            link = norm_text(e.link)
            published = getattr(e, "published", "")

            dt = safe_parse_date(published)

            company = detect_company(title)
            if company == "Unknown":
                continue

            tag = classify_tag(title)

            rows.append(
                {
                    "ê¸°ì—…ëª…": company,
                    "ì£¼(State)": detect_state(title),
                    "ë‰´ìŠ¤ ë°œí–‰ì¼": dt.strftime("%Y.%m.%d") if dt else "",
                    "í•µì‹¬ ë‚´ìš©": f"{tag} {title}",
                    "ì›ë¬¸ í™•ì¸": link,
                }
            )

    return rows


# ============================
# ê¸°ì—…ë‹¹ ìµœì‹  1ê°œë§Œ ì„ íƒ
# ============================
def pick_latest_per_company(rows):
    df = pd.DataFrame(rows)

    if df.empty:
        return df

    df["ë‰´ìŠ¤ ë°œí–‰ì¼_dt"] = pd.to_datetime(df["ë‰´ìŠ¤ ë°œí–‰ì¼"], errors="coerce")
    df = df.sort_values("ë‰´ìŠ¤ ë°œí–‰ì¼_dt", ascending=False)

    # ê¸°ì—…ë‹¹ ìµœì‹  1ê°œ
    top = df.groupby("ê¸°ì—…ëª…", as_index=False).head(1)

    # ìµœëŒ€ 10ê°œ ê¸°ì—…ë§Œ
    top = top.head(MAX_COMPANY_SHOW)

    return top.drop(columns=["ë‰´ìŠ¤ ë°œí–‰ì¼_dt"])


# ============================
# Streamlit UI
# ============================
st.set_page_config(page_title="ë¯¸êµ­ ì§„ì¶œ í•œêµ­ê¸°ì—… ë‰´ìŠ¤ ìƒí™©íŒ", layout="wide")

st.title("ğŸ“° ë¯¸êµ­ ë™ë‚¨ë¶€ ì§„ì¶œ í•œêµ­ê¸°ì—… ë‰´ìŠ¤ TOP10 ìƒí™©íŒ")
st.caption("ê¸°ì—…ë‹¹ ìµœì‹  ë‰´ìŠ¤ 1ê°œì”© ìë™ í‘œì‹œ + ì›ë¬¸ ë§í¬ í´ë¦­ ê°€ëŠ¥")

tab1, tab2 = st.tabs(["ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë‰´ìŠ¤", "ğŸ‡ºğŸ‡¸ ë¯¸êµ­ë°œ ë‰´ìŠ¤/ì£¼ì •ë¶€ í¬í•¨"])


# ----------------------------
# í•œêµ­ì–´ ë‰´ìŠ¤ íƒ­
# ----------------------------
with tab1:
    st.subheader("ëŒ€ê¸°ì—… ìµœì‹  ë‰´ìŠ¤ (ê¸°ì—…ë‹¹ 1ê°œ)")

    rows = fetch_google_news(korean_queries, provider="KOREAN")
    top = pick_latest_per_company(rows)

    if top.empty:
        st.warning("ê¸°ì—… ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. config.yaml ê¸°ì—…ëª…ì„ í™•ì¥í•˜ì„¸ìš”.")
    else:
        st.dataframe(
            top,
            use_container_width=True,
            hide_index=True,
            column_config={
                "ì›ë¬¸ í™•ì¸": st.column_config.LinkColumn("ì›ë¬¸ í™•ì¸")
            },
        )


# ----------------------------
# ë¯¸êµ­ ë‰´ìŠ¤ íƒ­
# ----------------------------
with tab2:
    st.subheader("ë¯¸êµ­ ì£¼ì •ë¶€/í˜„ì§€ì–¸ë¡  í¬í•¨ ìµœì‹  ë‰´ìŠ¤ (ê¸°ì—…ë‹¹ 1ê°œ)")

    rows = fetch_google_news(us_queries, provider="US")
    top = pick_latest_per_company(rows)

    if top.empty:
        st.warning("ë¯¸êµ­ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. us_queriesë¥¼ í™•ì¥í•˜ì„¸ìš”.")
    else:
        st.dataframe(
            top,
            use_container_width=True,
            hide_index=True,
            column_config={
                "ì›ë¬¸ í™•ì¸": st.column_config.LinkColumn("ì›ë¬¸ í™•ì¸")
            },
        )

st.markdown("---")
st.write("âœ… í‘œì‹œ ë°©ì‹: ê¸°ì—…ë‹¹ ìµœì‹  ê¸°ì‚¬ 1ê°œ + íˆ¬ì/ìˆ˜ì£¼/ì¦ì/íŒë§¤ê¸°ë¡ ìë™ íƒœê·¸ + ì›ë¬¸ ë§í¬ ì œê³µ")
