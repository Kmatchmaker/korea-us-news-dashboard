import re
import hashlib
from urllib.parse import urljoin, urlparse
from datetime import timezone

import pandas as pd
import requests
import streamlit as st
import yaml
from bs4 import BeautifulSoup
from dateutil import parser as dateparser


# ============================
# SETTINGS
# ============================
CONFIG_PATH = "config.yaml"
USER_AGENT = "Mozilla/5.0 (StreamlitNewsBoard/USGovOnly/4.0)"
HEADERS = {"User-Agent": USER_AGENT}

CACHE_TTL_SEC = 60 * 20
DEFAULT_YEAR_FILTER = 2026

TOP_COMPANY_MAX = 10   # TOP ê¸°ì—… ì„¹ì…˜ì—ì„œ ìµœëŒ€ 10ê°œ ê¸°ì—…
OTHER_MAX = 30         # ê¸°íƒ€(ì‹ ê·œ íˆ¬ì/ì§„ì¶œ/í™•ì¥) í‘œì‹œ ê°œìˆ˜

SIMILARITY_THRESHOLD = 0.86  # ìœ ì‚¬ ê¸°ì‚¬ ì œê±° ê°•ë„(0.80~0.92)


# ============================
# LOAD CONFIG
# ============================
def load_config():
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


cfg = load_config()
states_cfg = cfg.get("states", {})
priority_companies = cfg.get("priority_companies", ["í˜„ëŒ€", "SK", "LG", "í•œí™”", "ê³ ë ¤ì•„ì—°"])
us_sources = cfg.get("us_sources", [])


# ============================
# TEXT UTILS
# ============================
_ws = re.compile(r"\s+")
_punct = re.compile(r"[^0-9A-Za-zê°€-í£ .:/_-]+")
_digits = re.compile(r"\b\d+(?:[.,]\d+)*\b")


def norm_text(s: str) -> str:
    return _ws.sub(" ", (s or "").strip())


def safe_parse_date(s: str):
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        if dt and not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None


def make_id(provider: str, title: str, url: str) -> str:
    raw = f"{provider}||{norm_text(title)}||{norm_text(url)}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


# ============================
# STATE DETECTION (ì˜¤íƒ ìµœì†Œí™”)
# ============================
STATE_ABBR = ["GA", "TN", "AL", "SC", "FL"]
STATE_ABBR_RE = {abbr: re.compile(rf"(?<![A-Z0-9]){abbr}(?![A-Z0-9])") for abbr in STATE_ABBR}


def detect_state_strict(text: str, source_url: str) -> str:
    t = norm_text(text)
    tl = t.lower()

    # 1) ê¸´ ì´ë¦„ ë¨¼ì €
    for code, names in (states_cfg or {}).items():
        for n in names:
            nn = norm_text(str(n))
            if nn.upper() in STATE_ABBR:
                continue
            if nn and nn.lower() in tl:
                return code

    # 2) ì•½ì–´ëŠ” ë‹¨ë… í† í°ì¼ ë•Œë§Œ
    for abbr, rx in STATE_ABBR_RE.items():
        if rx.search(t):
            return abbr

    # 3) ë„ë©”ì¸ íŒíŠ¸(ì†ŒìŠ¤ ê¸°ì¤€)
    host = (urlparse(source_url).netloc or "").lower()
    if "gov.georgia.gov" in host or "georgia.org" in host:
        return "GA"
    if "tnecd.com" in host:
        return "TN"
    if "madeinalabama.com" in host:
        return "AL"
    if "sccommerce.com" in host:
        return "SC"
    if "floridajobs.org" in host:
        return "FL"

    return "Global"


# ============================
# COMPANY DETECTION (ì ˆëŒ€ 'ê¸°íƒ€' ê¸ˆì§€)
# ============================
TOP5_ALIASES = {
    "í˜„ëŒ€": ["í˜„ëŒ€", "í˜„ëŒ€ì°¨", "Hyundai", "ê¸°ì•„", "Kia"],
    "SK": ["SK", "SKì˜¨", "SK hynix", "SKí•˜ì´ë‹‰ìŠ¤", "í•˜ì´ë‹‰ìŠ¤", "SK Innovation", "SKì´ë…¸ë² ì´ì…˜"],
    "LG": ["LG", "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "LG Energy Solution", "LGí™”í•™", "LG Chem"],
    "í•œí™”": ["í•œí™”", "Hanwha", "í•œí™”íì…€", "Qcells", "Q CELLS"],
    "ê³ ë ¤ì•„ì—°": ["ê³ ë ¤ì•„ì—°", "Korea Zinc", "KoreaZinc"],
}

COMPANY_SUFFIX_HINT = re.compile(
    r"(ì¤‘ê³µì—…|ê¸ˆì†|ì˜¤í† |EPC|ì „ì|ì—ë„ˆì§€|í™”í•™|ê±´ì„¤|ëª¨ë¹Œë¦¬í‹°|í…Œí¬|ì‚°ì—…|ì†Œì¬|ì „ê¸°|ë°”ì´ì˜¤)$"
)

STOPWORDS = {
    # ì¸ë¬¼/ì§í•¨/ê¸°ê´€ ì„±ê²©
    "gov", "gov.", "governor", "office", "official", "statement", "commissioner", "deputy",
    "press", "release", "news", "department", "commerce", "economic", "development", "authority",
    # ì§€ì—­/ì¼ë°˜
    "us", "u.s", "usa", "america", "american",
    "georgia", "tennessee", "alabama", "florida", "carolina", "south", "north",
    "ë¯¸êµ­", "í•œêµ­", "ì¡°ì§€ì•„", "í…Œë„¤ì‹œ", "ì•¨ë¼ë°°ë§ˆ", "ì•Œë¼ë°°ë§ˆ", "í”Œë¡œë¦¬ë‹¤", "ì‚¬ìš°ìŠ¤ìºë¡¤ë¼ì´ë‚˜", "ìºë¡¤ë¼ì´ë‚˜",
    "ì£¼ì •ë¶€", "ì •ë¶€", "ìœ„ì›íšŒ", "ê²½ì œê°œë°œ", "ì¹´ìš´í‹°", "county", "city", "state",
    # í–‰ë™ ë‹¨ì–´
    "invest", "investment", "invests", "announce", "announces", "announced",
    "expansion", "expand", "contract", "agreement", "facility", "plant", "factory",
    "íˆ¬ì", "ê³µì¥", "ì„¤ë¦½", "ì¦ì„¤", "í™•ì¥", "ì§„ì¶œ", "ê³„ì•½", "ìˆ˜ì£¼", "ê³µê¸‰", "ì²´ê²°", "í˜‘ì•½",
}


def detect_company_from_title(title: str) -> str:
    t = norm_text(title)

    # 1) TOP5 alias ìš°ì„ 
    for canon, aliases in TOP5_ALIASES.items():
        for a in aliases:
            if a and a in t:
                return canon

    # 2) ì œëª© ë§¨ ì• í† í°
    m = re.match(r"^([ê°€-í£A-Za-z0-9&/.\-]{2,40})", t)
    if m:
        cand = m.group(1).strip(".,:-â€“â€”")
        cl = cand.lower()
        if cand and (cl not in STOPWORDS) and cl not in {"the", "a", "an"}:
            return cand

    # 3) í† í° í›„ë³´ë“¤
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9&/.\-]{2,40}", t)
    cleaned = []
    for tok in tokens:
        tok2 = tok.strip(".,:-â€“â€”()[]{}\"'")
        if not tok2:
            continue
        if tok2.lower() in STOPWORDS:
            continue
        cleaned.append(tok2)

    # 3-1) ì ‘ë¯¸ íŒíŠ¸ ìš°ì„ 
    for tok in cleaned:
        if COMPANY_SUFFIX_HINT.search(tok):
            return tok

    # 3-2) ì˜ë¬¸ íšŒì‚¬ ì ‘ë¯¸ ìš°ì„ 
    for tok in cleaned:
        if re.search(r"(Inc\.?|LLC|L\.L\.C\.|Corp\.?|Corporation|Co\.?|Company)$", tok, re.IGNORECASE):
            return tok

    # 3-3) ë‚¨ì€ ê²ƒ ì¤‘ ì²« í›„ë³´
    if cleaned:
        return cleaned[0]

    return "ë¯¸í™•ì¸"


def icon_for_company(company_plain: str) -> str:
    return "ğŸ‘‘" if company_plain in priority_companies else "ğŸ’"


# ============================
# TAG / IMPORTANCE
# ============================
INVEST_EN = ["invest", "investment", "plant", "facility", "expansion", "factory", "site", "build", "construct", "manufacturing"]
INVEST_KO = ["íˆ¬ì", "ê³µì¥", "ì„¤ë¦½", "ì¦ì„¤", "í™•ì¥", "ì§„ì¶œ", "ì‹ ê·œ"]
DEAL_EN = ["contract", "deal", "supply", "agreement", "award", "wins", "signed"]
DEAL_KO = ["ìˆ˜ì£¼", "ê³„ì•½", "ê³µê¸‰", "ì²´ê²°", "í˜‘ì•½", "íŒŒíŠ¸ë„ˆì‹­"]
CAPITAL_KO = ["ì¦ì", "ì¶œì", "ê³µì‹œ"]
SALES_KO = ["íŒë§¤", "ê¸°ë¡", "ëŒíŒŒ", "ë§¤ì¶œ", "ì‹¤ì "]


def classify_tag(text: str) -> str:
    tl = text.lower()
    if any(k in text for k in INVEST_KO) or any(k in tl for k in INVEST_EN):
        return "[ì‹ ê·œ íˆ¬ì]"
    if any(k in text for k in DEAL_KO) or any(k in tl for k in DEAL_EN):
        return "[ìˆ˜ì£¼/ê³„ì•½]"
    if any(k in text for k in CAPITAL_KO):
        return "[ìë³¸/ê³µì‹œ]"
    if any(k in text for k in SALES_KO):
        return "[ì‹¤ì /íŒë§¤]"
    return "[ì£¼ìš”]"


def importance_score(title: str, company_plain: str) -> int:
    score = 0
    if company_plain in priority_companies:
        score += 100
    tag = classify_tag(title)
    if tag == "[ì‹ ê·œ íˆ¬ì]":
        score += 35
    elif tag == "[ìˆ˜ì£¼/ê³„ì•½]":
        score += 25
    elif tag == "[ìë³¸/ê³µì‹œ]":
        score += 20
    elif tag == "[ì‹¤ì /íŒë§¤]":
        score += 15
    else:
        score += 5
    return score


# ============================
# DEDUP: ìœ ì‚¬ ê¸°ì‚¬ ì œê±° (Jaccard)
# ============================
def title_signature(title: str, company_plain: str) -> set:
    s = norm_text(title)
    if company_plain:
        s = s.replace(company_plain, " ")
    s = _digits.sub(" ", s)
    s = _punct.sub(" ", s)
    s = norm_text(s).lower()
    toks = [t for t in s.split() if t and t not in STOPWORDS and len(t) >= 2]
    return set(toks)


def jaccard(a: set, b: set) -> float:
    if not a and not b:
        return 1.0
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union else 0.0


def dedup_similar(rows: list[dict]) -> list[dict]:
    kept = []
    kept_sigs = []
    rows_sorted = sorted(rows, key=lambda r: (r["_when_sort"], r["_score"]), reverse=True)

    for r in rows_sorted:
        sig = r["_sig"]
        dup = False
        for ks in kept_sigs:
            if jaccard(sig, ks) >= SIMILARITY_THRESHOLD:
                dup = True
                break
        if not dup:
            kept.append(r)
            kept_sigs.append(sig)

    return kept


# ============================
# SOURCE-SPECIFIC PARSERS
# ============================
MONTH_RX_LONG = re.compile(r"(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}")
MONTH_RX_SHORT = re.compile(r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}")
DOT_DATE_RX = re.compile(r"\b\d{2}\.\d{2}\.\d{4}\b")  # TNECD ë©”ì¸ "Recent News"ì— ìì£¼ ë“±ì¥


def _is_probably_article_url(u: str) -> bool:
    # pdf ë“± ì œì™¸
    path = (urlparse(u).path or "").lower()
    if path.endswith(".pdf"):
        return False
    if "/docs/" in path or "/default-source/" in path:
        return False
    return True


def guess_items_from_page(html: str, base_url: str, max_items: int = 200):
    """
    ì†ŒìŠ¤ë³„ë¡œ 'ë‰´ìŠ¤/ë³´ë„ìë£Œ ìƒì„¸ ë§í¬' íŒ¨í„´ì„ ìš°ì„  ì ìš©í•´ ì¡ë§í¬ë¥¼ ìµœì†Œí™”í•œë‹¤.
    ë°˜í™˜: (title, url, date_text_or_none)
    """
    soup = BeautifulSoup(html, "html.parser")
    host = (urlparse(base_url).netloc or "").lower()
    items = []

    # -----------------------------
    # (GA) gov.georgia.gov : /press-releases/YYYY-MM-DD/slug
    # -----------------------------
    if "gov.georgia.gov" in host:
        for a in soup.select('a[href*="/press-releases/"]'):
            href = a.get("href", "")
            full_url = urljoin(base_url, href)
            if not re.search(r"/press-releases/\d{4}-\d{2}-\d{2}/", full_url):
                continue
            if not _is_probably_article_url(full_url):
                continue

            text = norm_text(a.get_text(" "))
            if not text or len(text) < 12:
                continue

            m = MONTH_RX_LONG.search(text)
            date_text = m.group(0) if m else None
            title = text.replace(date_text, "").strip() if date_text else text

            items.append((title, full_url, date_text))

    # -----------------------------
    # (GA) georgia.org : /press-releases/... (ì™¸ë¶€ ë§í¬ê°€ ì„ì¼ ìˆ˜ ìˆìŒ)
    # -----------------------------
    elif "georgia.org" in host:
        main = soup.select_one("main") or soup
        for a in main.select('a[href*="/press-releases"]'):
            href = a.get("href", "")
            full_url = urljoin(base_url, href)
            if not _is_probably_article_url(full_url):
                continue

            text = norm_text(a.get_text(" "))
            if not text or text.lower() in {"read more", "learn more"}:
                parent = a.find_parent()
                text = norm_text(parent.get_text(" ")) if parent else text

            m = MONTH_RX_SHORT.search(text)
            date_text = m.group(0) if m else None
            title = text.replace(date_text, "").replace("Read More", "").strip() if date_text else text

            if len(title) < 12:
                continue

            items.append((title, full_url, date_text))

    # -----------------------------
    # (TN) tnecd.com : /news/slug (ë©”ì¸ì—ë„ Recent Newsê°€ ìˆìŒ)
    # -----------------------------
    elif "tnecd.com" in host:
        main = soup.select_one("main") or soup

        # 1) /news/ ë§í¬ ìš°ì„ 
        for a in main.select('a[href*="/news/"]'):
            href = a.get("href", "")
            full_url = urljoin(base_url, href)
            if not _is_probably_article_url(full_url):
                continue

            text = norm_text(a.get_text(" "))
            if len(text) < 10:
                continue

            # ê°™ì€ ë¸”ë¡(ë¶€ëª¨)ì—ì„œ 02.04.2026 ê°™ì€ ë‚ ì§œë¥¼ ì°¾ì•„ë³´ê¸°
            parent = a.find_parent()
            ptxt = norm_text(parent.get_text(" ")) if parent else ""
            m = DOT_DATE_RX.search(ptxt)
            date_text = m.group(0) if m else None

            # DOT ë‚ ì§œëŠ” parseê°€ ì• ë§¤í•˜ë‹ˆ fetch ë‹¨ê³„ì—ì„œ ê·¸ëŒ€ë¡œ ë„˜ê¹€(ë‚˜ì¤‘ safe_parse_dateë¡œ ì²˜ë¦¬)
            items.append((text, full_url, date_text))

        # 2) ê·¸ë˜ë„ ë¶€ì¡±í•˜ë©´ /wp-content/ ê°™ì€ ê±´ ì œì™¸í•˜ê³  ì˜ë¯¸ìˆëŠ” ë§í¬ ì¶”ê°€
        if not items:
            for a in main.select("a[href]"):
                href = a.get("href", "")
                full_url = urljoin(base_url, href)
                if "/news/" not in full_url:
                    continue
                if not _is_probably_article_url(full_url):
                    continue
                text = norm_text(a.get_text(" "))
                if len(text) >= 10:
                    items.append((text, full_url, None))

    # -----------------------------
    # (AL) madeinalabama.com : /news/slug í˜•íƒœê°€ ë§ìŒ
    # -----------------------------
    elif "madeinalabama.com" in host:
        main = soup.select_one("main") or soup
        for a in main.select('a[href*="/news/"]'):
            href = a.get("href", "")
            full_url = urljoin(base_url, href)
            if not _is_probably_article_url(full_url):
                continue

            text = norm_text(a.get_text(" "))
            if len(text) < 10:
                continue

            # ë¶€ëª¨ì—ì„œ ë‚ ì§œ ì‹œë„
            parent = a.find_parent()
            ptxt = norm_text(parent.get_text(" ")) if parent else ""
            m = MONTH_RX_LONG.search(ptxt) or MONTH_RX_SHORT.search(ptxt)
            date_text = m.group(0) if m else None

            items.append((text, full_url, date_text))

    # -----------------------------
    # (SC) sccommerce.com/news : /news/... ë˜ëŠ” /news-... í˜•íƒœê°€ ì„ì¼ ìˆ˜ ìˆì–´ ë„“ê²Œ ì¡ë˜ main ìœ„ì£¼
    # -----------------------------
    elif "sccommerce.com" in host:
        main = soup.select_one("main") or soup
        for a in main.select("a[href]"):
            href = a.get("href", "")
            full_url = urljoin(base_url, href)
            path = (urlparse(full_url).path or "").lower()
            if "/news" not in path:
                continue
            if not _is_probably_article_url(full_url):
                continue

            text = norm_text(a.get_text(" "))
            if len(text) < 10:
                continue

            parent = a.find_parent()
            ptxt = norm_text(parent.get_text(" ")) if parent else ""
            m = MONTH_RX_LONG.search(ptxt) or MONTH_RX_SHORT.search(ptxt)
            date_text = m.group(0) if m else None

            items.append((text, full_url, date_text))

    # -----------------------------
    # (FL) floridajobs.org (DEO Press)
    # - docs/default-source pdf ë§í¬ê°€ ë§ì•„ì„œ ê°•í•˜ê²Œ ì œì™¸
    # - news-center ì•„ë˜ HTML ë§í¬ë§Œ ìš°ì„ 
    # -----------------------------
    elif "floridajobs.org" in host:
        main = soup.select_one("main") or soup
        for a in main.select("a[href]"):
            href = a.get("href", "")
            full_url = urljoin(base_url, href)
            if not _is_probably_article_url(full_url):
                continue

            path = (urlparse(full_url).path or "").lower()
            # news-center ë‚´ë¶€ë§Œ ìš°ì„ 
            if "/news-center" not in path:
                continue

            text = norm_text(a.get_text(" "))
            if len(text) < 10:
                continue

            parent = a.find_parent()
            ptxt = norm_text(parent.get_text(" ")) if parent else ""
            m = MONTH_RX_LONG.search(ptxt) or MONTH_RX_SHORT.search(ptxt)
            date_text = m.group(0) if m else None

            items.append((text, full_url, date_text))

    # -----------------------------
    # Fallback (ê·¸ ì™¸ ì†ŒìŠ¤ê°€ ì¶”ê°€ë˜ë”ë¼ë„ ìµœì†Œí•œ ë™ì‘)
    # -----------------------------
    else:
        # article ìš°ì„ 
        for art in soup.select("article"):
            a = art.select_one("a[href]")
            if not a:
                continue
            title = norm_text(a.get_text(" "))
            href = norm_text(a.get("href", ""))
            if not title or not href:
                continue
            full_url = urljoin(base_url, href)
            if not _is_probably_article_url(full_url):
                continue

            date_text = None
            time_tag = art.select_one("time")
            if time_tag:
                date_text = norm_text(time_tag.get("datetime") or time_tag.get_text(" "))

            if len(title) >= 10:
                items.append((title, full_url, date_text))

        if not items:
            main = soup.select_one("main") or soup
            for a in main.select("a[href]"):
                title = norm_text(a.get_text(" "))
                href = norm_text(a.get("href", ""))
                if not title or not href:
                    continue
                if len(title) < 10:
                    continue
                full_url = urljoin(base_url, href)
                if not _is_probably_article_url(full_url):
                    continue
                items.append((title, full_url, None))

    # Dedup + cap
    seen = set()
    out = []
    for t, u, d in items:
        key = (t, u)
        if key in seen:
            continue
        seen.add(key)
        out.append((t, u, d))
        if len(out) >= max_items:
            break

    return out


# ============================
# FETCH: US GOV SOURCES ONLY
# ============================
@st.cache_data(ttl=CACHE_TTL_SEC)
def fetch_us_gov_only(sources: list[dict]):
    raw_rows = []

    for src in sources:
        name = src.get("name", "US Government Source")
        url = src.get("url")
        allow_external = bool(src.get("allow_external", False))
        if not url:
            continue

        src_host = (urlparse(url).netloc or "").lower()

        try:
            r = requests.get(url, headers=HEADERS, timeout=25)
            r.raise_for_status()
            items = guess_items_from_page(r.text, url, max_items=200)
        except Exception:
            continue

        for title, link, date_text in items:
            link_host = (urlparse(link).netloc or "").lower()

            # ì™¸ë¶€ ë„ë©”ì¸ ë§í¬ ì œê±°(ê¸°ë³¸)
            if (not allow_external) and src_host and link_host and (src_host not in link_host):
                continue

            # ë‚ ì§œ íŒŒì‹± (TNECD "02.04.2026" ì¼€ì´ìŠ¤ ë³´ì •)
            dt = None
            if date_text:
                # 02.04.2026 í˜•ì‹ì´ë©´ month.day.yearë¡œ ê°€ì •(ì‚¬ì´íŠ¸ í‘œì‹œê°€ mm.dd.yyyy)
                if DOT_DATE_RX.fullmatch(date_text):
                    try:
                        mm, dd, yyyy = date_text.split(".")
                        dt = safe_parse_date(f"{yyyy}-{mm}-{dd}")
                    except Exception:
                        dt = None
                else:
                    dt = safe_parse_date(date_text)

            company_plain = detect_company_from_title(title)
            company_display = f"{icon_for_company(company_plain)} {company_plain}"

            state = detect_state_strict(title, url)
            tag = classify_tag(title)
            score = importance_score(title, company_plain)

            when_sort = pd.to_datetime(dt, errors="coerce", utc=True)
            if pd.isna(when_sort):
                # ë‚ ì§œ ì—†ìœ¼ë©´ ì˜¤ë˜ëœ ê²ƒìœ¼ë¡œ(ìµœì‹  ì •ë ¬ì—ì„œ ë°€ë¦¼)
                when_sort = pd.Timestamp.now(tz="UTC") - pd.Timedelta(days=3650)

            sig = title_signature(title, company_plain)

            raw_rows.append(
                {
                    "source": name,
                    "ì£¼(State)": state,
                    "ê¸°ì—…ëª…": company_display,
                    "ë‰´ìŠ¤ ë°œí–‰ì¼": when_sort.strftime("%Y.%m.%d"),
                    "í•µì‹¬ ë‚´ìš©": f"{tag} {title}",
                    "ì›ë¬¸ í™•ì¸": link,
                    "_score": score,
                    "_when_sort": when_sort,
                    "_sig": sig,
                }
            )

    # URL/ì œëª© ê¸°ë°˜ dedup
    dedup = {}
    for r in raw_rows:
        key = make_id("US_GOV", r["í•µì‹¬ ë‚´ìš©"], r["ì›ë¬¸ í™•ì¸"])
        dedup[key] = r
    rows = list(dedup.values())

    # ìœ ì‚¬ ê¸°ì‚¬ ì œê±°
    rows = dedup_similar(rows)

    # ìµœì¢… ì •ë ¬
    rows.sort(key=lambda r: (r["_when_sort"], r["_score"]), reverse=True)
    return rows


# ============================
# DISPLAY HELPERS
# ============================
def apply_year_filter_df(df: pd.DataFrame, year_filter):
    if df.empty:
        return df
    if year_filter == "ì „ì²´":
        return df
    y = str(int(year_filter))
    return df[df["ë‰´ìŠ¤ ë°œí–‰ì¼"].str.startswith(y)]


def plain_company(display_name: str) -> str:
    return norm_text(display_name.replace("ğŸ‘‘", "").replace("ğŸ’", ""))


def pick_top_company_one_each(df: pd.DataFrame, top_companies: list[str]) -> pd.DataFrame:
    if df.empty:
        return df

    d = df.copy()
    d["_plain"] = d["ê¸°ì—…ëª…"].apply(plain_company)

    top = d[d["_plain"].isin(top_companies)].copy()
    if top.empty:
        return top.drop(columns=["_plain"], errors="ignore")

    top = top.groupby("_plain", as_index=False).head(1)

    order = {c: i for i, c in enumerate(top_companies)}
    top["_order"] = top["_plain"].map(lambda x: order.get(x, 9999))
    top = top.sort_values("_order").head(TOP_COMPANY_MAX)

    return top.drop(columns=["_plain", "_order"], errors="ignore")


def pick_other(df: pd.DataFrame, top_companies: list[str], n: int) -> pd.DataFrame:
    if df.empty:
        return df
    d = df.copy()
    d["_plain"] = d["ê¸°ì—…ëª…"].apply(plain_company)
    other = d[~d["_plain"].isin(top_companies)].copy().head(n)
    return other.drop(columns=["_plain"], errors="ignore")


# ============================
# UI
# ============================
st.set_page_config(page_title="US ì£¼ì •ë¶€ ë‰´ìŠ¤ ì „ìš© ìƒí™©íŒ", layout="wide")
st.title("ğŸ‡ºğŸ‡¸ US ì£¼ì •ë¶€/ì£¼(å·) ì‚°í•˜ê¸°ê´€ ë‰´ìŠ¤ ì „ìš©: í•œêµ­ê¸°ì—… ì§„ì¶œÂ·í™•ì¥ ìƒí™©íŒ")
st.caption(
    "US íƒ­ì€ config.yamlì˜ us_sourcesë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. "
    "ì†ŒìŠ¤ë³„ ë§í¬ íŒ¨í„´ì„ ì ìš©í•´ ì¡ë§í¬ë¥¼ ì¤„ì˜€ê³ , ê¸°ì—…ëª…ì€ 'ê¸°íƒ€' ì—†ì´ ì œëª©ì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤. "
    "ìœ ì‚¬ ê¸°ì‚¬ëŠ” ìë™ ì œê±°ë©ë‹ˆë‹¤."
)

with st.sidebar:
    st.subheader("í•„í„°")
    year_filter = st.selectbox("ë°œí–‰ ì—°ë„", [DEFAULT_YEAR_FILTER, 2025, 2024, "ì „ì²´"], index=0)
    st.markdown("---")
    st.write("TOP5(ğŸ‘‘):")
    st.code(", ".join(priority_companies))
    if st.button("ğŸ”„ ìºì‹œ ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()
        st.rerun()

if not us_sources:
    st.warning("config.yamlì˜ us_sourcesê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì£¼ì •ë¶€/ê¸°ê´€ ë‰´ìŠ¤ URLì„ ë„£ì–´ì£¼ì„¸ìš”.")
else:
    rows = fetch_us_gov_only(us_sources)
    df = pd.DataFrame(rows)
    df = apply_year_filter_df(df, year_filter)

    st.subheader("â­ TOP ê¸°ì—… ìµœì‹ (ê¸°ì—…ë‹¹ 1ê°œ, ìµœëŒ€ 10ê°œ ê¸°ì—…)")
    top = pick_top_company_one_each(df, priority_companies)
    if top.empty:
        st.info("TOP ê¸°ì—… ê´€ë ¨ ì£¼ì •ë¶€/ê¸°ê´€ ë‰´ìŠ¤ê°€ ì•„ì§ ì—†ê±°ë‚˜ ê¸°ì—…ëª… ì¶”ì¶œ/alias í™•ì¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    st.dataframe(
        top[["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ì›ë¬¸ í™•ì¸"]] if not top.empty else top,
        use_container_width=True,
        hide_index=True,
        column_config={"ì›ë¬¸ í™•ì¸": st.column_config.LinkColumn("ì›ë¬¸ í™•ì¸")},
    )

    st.subheader("ğŸ†• ì‹ ê·œ íˆ¬ì/ì§„ì¶œ/í™•ì¥ (ìµœì‹ , ìœ ì‚¬ ê¸°ì‚¬ ì œê±°ë¨)")
    other = pick_other(df, priority_companies, OTHER_MAX)
    st.dataframe(
        other[["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ì›ë¬¸ í™•ì¸"]] if not other.empty else other,
        use_container_width=True,
        hide_index=True,
        column_config={"ì›ë¬¸ í™•ì¸": st.column_config.LinkColumn("ì›ë¬¸ í™•ì¸")},
    )
