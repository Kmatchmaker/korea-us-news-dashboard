import re
import hashlib
from datetime import datetime, timezone
from urllib.parse import quote, urljoin

import pandas as pd
import requests
import streamlit as st
import feedparser
import yaml
from bs4 import BeautifulSoup
from dateutil import parser as dateparser


# ============================
# SETTINGS
# ============================
CONFIG_PATH = "config.yaml"
USER_AGENT = "Mozilla/5.0 (StreamlitNewsBoard/3.0)"
HEADERS = {"User-Agent": USER_AGENT}

DEFAULT_YEAR_FILTER = 2026  # ê¸°ë³¸ 2026ë…„ (ì›í•˜ë©´ "ì „ì²´"ë¡œ ë°”ê¿”ë„ ë¨)

TOP5_MAX = 10               # TOP ì„¹ì…˜ì—ì„œ ìµœëŒ€ ê¸°ì—… ìˆ˜(ìš”ì²­: ìµœëŒ€ 10ê°œ ê¸°ì—… ë³´ì´ê¸°)
OTHER_MAX = 20              # ê¸°íƒ€(ì‹ ê·œ íˆ¬ì/ì§„ì¶œ/ì‚¬ì—…í˜„í™©) í‘œì‹œ ê°œìˆ˜
CACHE_TTL_SEC = 60 * 20     # 20ë¶„ ìºì‹œ


# ============================
# LOAD CONFIG
# ============================
def load_config():
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


cfg = load_config()

states_cfg = cfg.get("states", {})  # ex) GA: ["Georgia","ì¡°ì§€ì•„","GA"] í˜•íƒœ
priority_companies = cfg.get("priority_companies", ["í˜„ëŒ€", "SK", "LG", "í•œí™”", "ê³ ë ¤ì•„ì—°"])

korean_queries = cfg.get("korean_queries", [])
us_sources = cfg.get("us_sources", [])  # (ì„ íƒ) ì£¼ì •ë¶€/ê¸°ê´€ í˜ì´ì§€ë“¤
us_queries = cfg.get(
    "us_queries",
    [
        '(Georgia OR Tennessee OR Alabama OR Florida OR "South Carolina" OR GA OR TN OR AL OR FL OR SC) '
        '(Korean OR Korea OR "South Korean" OR "í•œêµ­") '
        '(investment OR invest OR plant OR factory OR expansion OR contract OR subsidiary OR announce OR "economic development") '
        '(Hyundai OR SK OR LG OR Hanwha OR "Korean company" OR supplier)'
    ],
)


# ============================
# TEXT UTILS
# ============================
_ws = re.compile(r"\s+")
_html_tag = re.compile(r"<[^>]+>")


def norm_text(s: str) -> str:
    return _ws.sub(" ", (s or "").strip())


def strip_html(s: str) -> str:
    return norm_text(_html_tag.sub(" ", s or ""))


def norm_query_for_url(q: str) -> str:
    # ì¤„ë°”ê¿ˆ/ë‹¤ì¤‘ê³µë°± ì œê±° í›„ URL ì¸ì½”ë”©
    return quote(norm_text(q))


def safe_parse_date(s: str):
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        if dt and not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None


def to_display_date(dt) -> str:
    if not dt:
        return ""
    try:
        return dt.astimezone(timezone.utc).strftime("%Y.%m.%d")
    except Exception:
        try:
            return dt.strftime("%Y.%m.%d")
        except Exception:
            return ""


def make_id(provider: str, title: str, url: str) -> str:
    raw = f"{provider}||{norm_text(title)}||{norm_text(url)}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


# ============================
# STATE DETECTION
# ============================
def detect_state(text: str) -> str:
    t = norm_text(text).lower()
    for code, names in (states_cfg or {}).items():
        # names can be list or dict(names=[...])
        if isinstance(names, dict):
            names_list = names.get("names", [])
        else:
            names_list = names
        for n in names_list:
            if norm_text(str(n)).lower() in t:
                return code
    return "Global"


# ============================
# COMPANY DETECTION (NO MANUAL 100 LIST)
# - TOP5ëŠ” í™•ì‹¤íˆ ìºì¹˜
# - ê·¸ ì™¸ëŠ” í•œêµ­ ê¸°ì—…ëª… íŒ¨í„´ìœ¼ë¡œ ìë™ ì¶”ì¶œ
# ============================
# TOP5 í‘œê¸° í†µì¼(íƒ€ì´í‹€ì—ì„œ ë‹¤ì–‘í•œ í‘œê¸°ë¥¼ í•œ ì´ë¦„ìœ¼ë¡œ ë¬¶ê¸°)
TOP5_ALIASES = {
    "í˜„ëŒ€": ["í˜„ëŒ€", "í˜„ëŒ€ì°¨", "Hyundai", "HYUNDAI", "ê¸°ì•„", "Kia", "KIA"],
    "SK": ["SK", "SKì˜¨", "SKì˜¨", "SK hynix", "SKí•˜ì´ë‹‰ìŠ¤", "í•˜ì´ë‹‰ìŠ¤", "SKì´ë…¸ë² ì´ì…˜", "SK Innovation"],
    "LG": ["LG", "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "LG Energy Solution", "LGí™”í•™", "LG Chem"],
    "í•œí™”": ["í•œí™”", "Hanwha", "HANWHA", "í•œí™”íì…€", "Qcells", "Q CELLS"],
    "ê³ ë ¤ì•„ì—°": ["ê³ ë ¤ì•„ì—°", "Korea Zinc", "KoreaZinc", "KOREA ZINC"],
}

# ìë™ ì¶”ì¶œ íŒ¨í„´(ë„ˆë¬´ ê³µê²©ì ì´ë©´ ë…¸ì´ì¦ˆ ìƒê¸¸ ìˆ˜ ìˆì–´ì„œ â€œê¸°ì—…ëª…ê°™ì€ ê²ƒâ€ ìœ„ì£¼ë¡œ)
AUTO_PATTERNS = [
    r"([ê°€-í£A-Za-z]{2,20}ì „ì)",
    r"([ê°€-í£A-Za-z]{2,20}ì¤‘ê³µì—…)",
    r"([ê°€-í£A-Za-z]{2,20}ì‚°ì—…)",
    r"([ê°€-í£A-Za-z]{2,20}ì—ë„ˆì§€)",
    r"([ê°€-í£A-Za-z]{2,20}í™”í•™)",
    r"([ê°€-í£A-Za-z]{2,20}ê±´ì„¤)",
    r"([ê°€-í£A-Za-z]{2,20}ëª¨ë¹Œë¦¬í‹°)",
    r"([ê°€-í£A-Za-z]{2,20}í…Œí¬)",
    r"([ê°€-í£A-Za-z]{2,20}EPC)",
    r"([ê°€-í£A-Za-z]{2,20}ì˜¤í† )",
    r"([ê°€-í£A-Za-z]{2,20}ê¸ˆì†)",
    r"([ê°€-í£A-Za-z]{2,20}ì†Œì¬)",
    r"([ê°€-í£A-Za-z]{2,20}ì „ê¸°)",
]


def detect_company_auto(title: str) -> str:
    t = norm_text(title)

    # 1) TOP5 alias ìš°ì„ 
    for canon, aliases in TOP5_ALIASES.items():
        for a in aliases:
            if a and a in t:
                return canon

    # 2) ìë™ íŒ¨í„´
    for p in AUTO_PATTERNS:
        m = re.search(p, t)
        if m:
            name = m.group(1)
            # ë„ˆë¬´ í”í•œ ë‹¨ì–´/ê¸°ê´€/ì§€ì—­ì´ ì¡íˆëŠ” ê²ƒ ë°©ì§€ (ê°€ë²¼ìš´ ì•ˆì „ì¥ì¹˜)
            if len(name) >= 2 and name not in ["í•œêµ­", "ë¯¸êµ­", "ì¡°ì§€ì•„", "í…Œë„¤ì‹œ", "í”Œë¡œë¦¬ë‹¤"]:
                return name

    return "ê¸°íƒ€ í•œêµ­ê¸°ì—…"


# ============================
# TAG / IMPORTANCE
# ============================
INVEST = ["íˆ¬ì", "ê³µì¥", "ì„¤ë¦½", "ì¦ì„¤", "ì§„ì¶œ", "í™•ì¥", "ì‹ ê·œ", "ë¼ì¸", "ìº í¼ìŠ¤"]
DEAL = ["ìˆ˜ì£¼", "ê³„ì•½", "ê³µê¸‰", "ì²´ê²°", "MOU", "í˜‘ì•½", "íŒŒíŠ¸ë„ˆì‹­"]
CAPITAL = ["ì¦ì", "ì¶œì", "ê³µì‹œ"]
SALES = ["íŒë§¤", "ê¸°ë¡", "ëŒíŒŒ", "ë§¤ì¶œ", "ì‹¤ì "]
GOV = ["ì •ë¶€", "ë²”ë¶€ì²˜", "ìœ„ì›íšŒ", "MOU ì´í–‰", "ì „ëµíˆ¬ì"]


def classify_tag(text: str) -> str:
    if any(k in text for k in GOV):
        return "[ì •ì±…/ì •ë¶€]"
    if any(k in text for k in INVEST):
        return "[ì‹ ê·œ íˆ¬ì]"
    if any(k in text for k in DEAL):
        return "[ìˆ˜ì£¼/ê³„ì•½]"
    if any(k in text for k in CAPITAL):
        return "[ìë³¸/ê³µì‹œ]"
    if any(k in text for k in SALES):
        return "[ì‹¤ì /íŒë§¤]"
    return "[ì£¼ìš”]"


def importance_score(title: str, provider: str, company: str) -> int:
    text = title
    score = 0
    if company in priority_companies:
        score += 100
    if any(k in text for k in GOV):
        score += 40
    if any(k in text for k in INVEST):
        score += 35
    if any(k in text for k in DEAL):
        score += 25
    if any(k in text for k in CAPITAL):
        score += 20
    if any(k in text for k in SALES):
        score += 15
    if provider == "KOREAN":
        score += 5
    return score


def icon_for_company(company: str) -> str:
    return "ğŸ‘‘" if company in priority_companies else "ğŸ’"


# ============================
# FETCH: Google News RSS (KR)
# ============================
@st.cache_data(ttl=CACHE_TTL_SEC)
def fetch_google_news_kr(queries: list[str], provider_label: str):
    rows = []
    for q in queries:
        q_encoded = norm_query_for_url(q)
        url = f"https://news.google.com/rss/search?q={q_encoded}&hl=ko&gl=KR&ceid=KR:ko"

        feed = feedparser.parse(url)
        for e in feed.entries[:60]:
            title = norm_text(getattr(e, "title", ""))
            link = norm_text(getattr(e, "link", ""))
            published_raw = getattr(e, "published", "") or getattr(e, "updated", "")
            dt = safe_parse_date(published_raw)

            summary_html = getattr(e, "summary", None) or getattr(e, "description", None)
            summary = strip_html(summary_html or "")

            if not title or not link:
                continue

            company = detect_company_auto(title)
            state = detect_state(title)

            tag = classify_tag(title)
            core = summary if summary else title
            core = (core[:180] + "â€¦") if len(core) > 180 else core

            rows.append(
                {
                    "provider": provider_label,
                    "source": "Google News (KR)",
                    "title": title,
                    "url": link,
                    "published_at": dt,
                    "state": state,
                    "company": company,
                    "tag": tag,
                    "core": core,
                    "score": importance_score(title, provider_label, company),
                }
            )

    # dedup
    dedup = {}
    for r in rows:
        dedup[make_id(r["provider"], r["title"], r["url"])] = r
    return list(dedup.values())


# ============================
# FETCH: US SOURCES (optional, HTML list)
# - ì£¼ì •ë¶€/ê¸°ê´€ ì‚¬ì´íŠ¸ëŠ” êµ¬ì¡°ê°€ ì œê°ê°ì´ë¼ "ëŒ€ëµì  ë§í¬ ë¦¬ìŠ¤íŠ¸" ì¶”ì¶œ
# - ì œëª©ë§Œ ê°€ì ¸ì˜¤ëŠ” ìˆ˜ì¤€(ìš”ì•½/ë°œí–‰ì¼ì€ ì‚¬ì´íŠ¸ë³„ ì œê°ê°)
# ============================
def guess_items_from_page(html: str, base_url: str, max_items: int = 50):
    soup = BeautifulSoup(html, "html.parser")
    items = []

    for art in soup.select("article"):
        a = art.select_one("a[href]")
        if not a:
            continue
        title = norm_text(a.get_text(" "))
        href = norm_text(a.get("href", ""))
        if not title or not href:
            continue
        full_url = urljoin(base_url, href)

        date_text = None
        time_tag = art.select_one("time")
        if time_tag:
            date_text = norm_text(time_tag.get("datetime") or time_tag.get_text(" "))

        items.append((title, full_url, date_text))

    if not items:
        for a in soup.select("a[href]"):
            title = norm_text(a.get_text(" "))
            href = norm_text(a.get("href", ""))
            if not title or not href:
                continue
            if len(title) < 12:
                continue
            full_url = urljoin(base_url, href)
            items.append((title, full_url, None))

    # dedup + cap
    seen = set()
    out = []
    for t, u, d in items:
        key = (t, u)
        if key in seen:
            continue
        seen.add(key)
        out.append((t, u, d))
        if len(out) >= max_items:
            break
    return out


@st.cache_data(ttl=CACHE_TTL_SEC)
def fetch_us_source_pages(sources: list[dict]):
    rows = []
    for src in sources:
        name = src.get("name", "US Source")
        url = src.get("url")
        if not url:
            continue
        try:
            r = requests.get(url, headers=HEADERS, timeout=20)
            r.raise_for_status()
            items = guess_items_from_page(r.text, url, max_items=40)
        except Exception:
            continue

        for title, link, date_text in items:
            dt = safe_parse_date(date_text) if date_text else None

            company = detect_company_auto(title)
            state = detect_state(title)

            tag = classify_tag(title)
            core = title
            core = (core[:180] + "â€¦") if len(core) > 180 else core

            rows.append(
                {
                    "provider": "US_PAGE",
                    "source": name,
                    "title": title,
                    "url": link,
                    "published_at": dt,
                    "state": state,
                    "company": company,
                    "tag": tag,
                    "core": core,
                    "score": importance_score(title, "US", company),
                }
            )

    dedup = {}
    for r in rows:
        dedup[make_id(r["provider"], r["title"], r["url"])] = r
    return list(dedup.values())


# ============================
# BUILD DISPLAY TABLES
# ============================
def build_df(rows: list[dict]) -> pd.DataFrame:
    if not rows:
        return pd.DataFrame(columns=["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ì›ë¬¸ í™•ì¸"])

    df = pd.DataFrame(rows)

    # datetime normalize for sorting
    df["_when"] = pd.to_datetime(df["published_at"], errors="coerce", utc=True)
    now = pd.Timestamp.now(tz="UTC")
    df["_when"] = df["_when"].fillna(now - pd.Timedelta(days=3650))

    df["_score"] = df["score"].fillna(0).astype(int)

    df["ë‰´ìŠ¤ ë°œí–‰ì¼"] = df["_when"].dt.strftime("%Y.%m.%d")
    df["ì£¼(State)"] = df["state"]
    df["ê¸°ì—…ëª…"] = df["company"].apply(lambda c: f"{icon_for_company(c)} {c}")
    df["í•µì‹¬ ë‚´ìš©"] = df.apply(lambda r: f"{r['tag']} {r['core']}", axis=1)

    # ë§í¬ëŠ” LinkColumnìœ¼ë¡œ í‘œì‹œí•  ê±°ë¼ URL ê·¸ëŒ€ë¡œ ë‘ 
    df["ì›ë¬¸ í™•ì¸"] = df["url"]

    # ìµœì‹ /ì¤‘ìš”ë„ ì •ë ¬
    df = df.sort_values(by=["_when", "_score"], ascending=[False, False])
    return df


def apply_year_filter(df: pd.DataFrame, year_filter):
    if df.empty:
        return df
    if year_filter == "ì „ì²´":
        return df
    y = str(int(year_filter))
    return df[df["ë‰´ìŠ¤ ë°œí–‰ì¼"].str.startswith(y)]


def pick_top_per_company(df: pd.DataFrame, top_companies: list[str]) -> pd.DataFrame:
    if df.empty:
        return df

    # í‘œì‹œëª…(ğŸ‘‘ í˜„ëŒ€) â†’ ì‹¤ì œ ë¹„êµëŠ” ì›ë³¸ companyë¡œ í•´ì•¼ í•˜ë¯€ë¡œ ì›ë³¸ ì—´ì´ í•„ìš”
    # ì—¬ê¸°ì„œëŠ” "company" ì›ë³¸ì´ dfì— ì—†ìœ¼ë‹ˆ, í‘œì‹œëª…ì—ì„œ ì œê±°
    def strip_icon(name: str) -> str:
        return norm_text(name.replace("ğŸ‘‘", "").replace("ğŸ’", ""))

    df2 = df.copy()
    df2["_company_plain"] = df2["ê¸°ì—…ëª…"].apply(strip_icon)

    subset = df2[df2["_company_plain"].isin(top_companies)].copy()
    if subset.empty:
        return subset.drop(columns=["_company_plain"], errors="ignore")

    # ê¸°ì—…ë‹¹ 1ê°œ
    subset = subset.groupby("_company_plain", as_index=False).head(1)

    # ìˆœì„œ: top_companies ìˆœì„œëŒ€ë¡œ
    order_map = {c: i for i, c in enumerate(top_companies)}
    subset["_order"] = subset["_company_plain"].map(lambda x: order_map.get(x, 9999))

    subset = subset.sort_values(by=["_order"], ascending=True)
    subset = subset.head(TOP5_MAX)

    return subset.drop(columns=["_company_plain", "_order"], errors="ignore")


def pick_other_updates(df: pd.DataFrame, top_companies: list[str], n: int) -> pd.DataFrame:
    if df.empty:
        return df

    def strip_icon(name: str) -> str:
        return norm_text(name.replace("ğŸ‘‘", "").replace("ğŸ’", ""))

    df2 = df.copy()
    df2["_company_plain"] = df2["ê¸°ì—…ëª…"].apply(strip_icon)

    other = df2[~df2["_company_plain"].isin(top_companies)].copy()
    if other.empty:
        return other.drop(columns=["_company_plain"], errors="ignore")

    # "ê¸°íƒ€ í•œêµ­ê¸°ì—…"ë„ ìµœì‹  íˆ¬ì/ì§„ì¶œ ê¸°ì‚¬ë©´ ê°€ì¹˜ê°€ ìˆìœ¼ë‹ˆ í¬í•¨
    other = other.head(n)
    return other.drop(columns=["_company_plain"], errors="ignore")


# ============================
# UI
# ============================
st.set_page_config(page_title="ë¯¸êµ­ ì§„ì¶œ í•œêµ­ê¸°ì—… ë‰´ìŠ¤ ìƒí™©íŒ", layout="wide")
st.title("ğŸ“° ë¯¸êµ­ ì§„ì¶œ í•œêµ­ê¸°ì—… ë‰´ìŠ¤ ìƒí™©íŒ")
st.caption("TOP5(í˜„ëŒ€/SK/LG/í•œí™”/ê³ ë ¤ì•„ì—°)ëŠ” ê¸°ì—…ë‹¹ 1ê°œ, ê·¸ ì™¸ëŠ” ìë™ìœ¼ë¡œ ê¸°ì—…ëª…ì„ ì¶”ì¶œí•´ ìµœì‹  ì—…ë°ì´íŠ¸ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")

with st.sidebar:
    st.subheader("í•„í„°")
    year_filter = st.selectbox("ë°œí–‰ ì—°ë„", [DEFAULT_YEAR_FILTER, 2025, 2024, "ì „ì²´"], index=0)
    st.markdown("---")
    st.write("TOP5(ê³ ì • í‘œì‹œ):")
    st.code(", ".join(priority_companies))
    if st.button("ğŸ”„ ìºì‹œ ìƒˆë¡œê³ ì¹¨(ê°•ì œ ì¬ìˆ˜ì§‘)"):
        st.cache_data.clear()
        st.rerun()

tab1, tab2 = st.tabs(["ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë‰´ìŠ¤", "ğŸ‡ºğŸ‡¸ ë¯¸êµ­(ì£¼ì •ë¶€/í˜„ì§€) ë‰´ìŠ¤"])

# ---- Tab 1: Korean news (KR)
with tab1:
    st.subheader("â­ TOP ê¸°ì—… ìµœì‹  (ê¸°ì—…ë‹¹ 1ê°œ)")
    rows_kr = fetch_google_news_kr(korean_queries, provider_label="KOREAN")
    df_kr = apply_year_filter(build_df(rows_kr), year_filter)

    top_kr = pick_top_per_company(df_kr, priority_companies)
    if top_kr.empty:
        st.info("TOP ê¸°ì—… ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. korean_queriesë¥¼ í™•ì¥í•´ë³´ì„¸ìš”.")
    else:
        st.dataframe(
            top_kr[["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ì›ë¬¸ í™•ì¸"]],
            use_container_width=True,
            hide_index=True,
            column_config={"ì›ë¬¸ í™•ì¸": st.column_config.LinkColumn("ì›ë¬¸ í™•ì¸")},
        )

    st.subheader("ğŸ†• ì‹ ê·œ íˆ¬ìÂ·ì§„ì¶œ ë° ë¯¸êµ­ ì‚¬ì—… í˜„í™© (ìë™ ì¶”ì¶œ ê¸°ì—…)")
    other_kr = pick_other_updates(df_kr, priority_companies, OTHER_MAX)
    if other_kr.empty:
        st.info("ì¶”ê°€ ì—…ë°ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.dataframe(
            other_kr[["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ì›ë¬¸ í™•ì¸"]],
            use_container_width=True,
            hide_index=True,
            column_config={"ì›ë¬¸ í™•ì¸": st.column_config.LinkColumn("ì›ë¬¸ í™•ì¸")},
        )

# ---- Tab 2: US news (mix: Google News query + optional state pages)
with tab2:
    st.subheader("â­ TOP ê¸°ì—… ìµœì‹  (ê¸°ì—…ë‹¹ 1ê°œ)")
    rows_us_gn = fetch_google_news_kr(us_queries, provider_label="US_GNEWS")
    rows_us_pages = fetch_us_source_pages(us_sources) if us_sources else []
    rows_us_all = rows_us_gn + rows_us_pages

    df_us = apply_year_filter(build_df(rows_us_all), year_filter)

    top_us = pick_top_per_company(df_us, priority_companies)
    if top_us.empty:
        st.info("TOP ê¸°ì—… ë¯¸êµ­ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. us_queries / us_sourcesë¥¼ í™•ì¥í•´ë³´ì„¸ìš”.")
    else:
        st.dataframe(
            top_us[["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ì›ë¬¸ í™•ì¸"]],
            use_container_width=True,
            hide_index=True,
            column_config={"ì›ë¬¸ í™•ì¸": st.column_config.LinkColumn("ì›ë¬¸ í™•ì¸")},
        )

    st.subheader("ğŸ†• ì‹ ê·œ íˆ¬ìÂ·ì§„ì¶œ ë° ë¯¸êµ­ ì‚¬ì—… í˜„í™© (ìë™ ì¶”ì¶œ ê¸°ì—…)")
    other_us = pick_other_updates(df_us, priority_companies, OTHER_MAX)
    if other_us.empty:
        st.info("ì¶”ê°€ ì—…ë°ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.dataframe(
            other_us[["ì£¼(State)", "ê¸°ì—…ëª…", "ë‰´ìŠ¤ ë°œí–‰ì¼", "í•µì‹¬ ë‚´ìš©", "ì›ë¬¸ í™•ì¸"]],
            use_container_width=True,
            hide_index=True,
            column_config={"ì›ë¬¸ í™•ì¸": st.column_config.LinkColumn("ì›ë¬¸ í™•ì¸")},
        )

st.markdown("---")
st.write("âœ… ê¸°ì—…ëª…ì€ TOP5ëŠ” ê³ ì •, ë‚˜ë¨¸ì§€ëŠ” ê¸°ì‚¬ ì œëª©ì—ì„œ ìë™ ì¶”ì¶œí•©ë‹ˆë‹¤. (ê¸°ì—…ëª… 100ê°œ ì…ë ¥í•  í•„ìš” ì—†ìŒ)")
